# Shader必备概念
## 渲染管线是什么
### 渲染管线概述

#### 什么是渲染管线？

渲染管线是计算机图形学中用于将三维场景转换为最终屏幕所见图像的过程，由一系列的阶段和操作组成，每一个阶段都负责执行特定的任务，逐步处理输入的集合数据和纹理信息，最终生成可视化图像。

用一句话总结就是：**渲染管线是将数据分阶段的变为屏幕图像的过程**。

#### 渲染管线中的数据

1. 顶点数据：模型的顶点坐标、法线向量、纹理坐标等
2. 纹理数据：纹理贴图等
3. 光照数据：光照参数、光源信息等

等等Unity场景上相关的数据。

#### 渲染管线中的阶段

1. 应用阶段
2. 几何阶段
3. 光栅化阶段

在每一个阶段都会对数据进行处理，最终目的就是在屏幕上让我们看见最终的图像。

### 渲染管线——应用阶段

#### 必备知识：CPU和GPU

- CPU：中央处理器，负责算数运算、逻辑操作、数据传输等通用计算任务，同时还管理和调度计算机的资源（游戏开发中一游戏逻辑处理）。
- GPU：图形处理器，是专门用于图形和并行计算的处理器。显卡就是搭载GPU的硬件设备，显卡包含一个或多个GPU芯片，还包含显存（用于存储图像数据）、显示接口、视频解码器等等（游戏开发中一渲染相关处理）。

CPU主要处理操作系统管理、程序执行、通用计算等等；GPU主要处理图形渲染、图像处理等等。

#### 渲染管线在应用阶段主要做什么

渲染管线的应用阶段中大部分的内容都和渲染无关（比如：游戏逻辑处理、动画更新、物理模拟、场景管理等等），当应用阶段完成后，后面的几何阶段以及光栅化阶段将开始处理和图形渲染相关的数据和操作。

那么应用阶段为什么会归纳到渲染管线中呢？

那是因为应用阶段为渲染管线的后续提供了最重要的内容一**数据**，应用阶段主导者是CPU，在这一阶段，我们**将渲染需要用到的数据传递给GPU用于后续的两个阶段的处理**。

#### 应用阶段为渲染准备了什么

1. 把不可见的物体数据剔除
2. 准备好模型相关数据（顶点、法线、切线、贴图、着色器等等）
3. 将数据加载到显存中
4. 设置渲染状态（设置网格需要使用那个着色器、材质、光源属性等等）
5. 调用DrawCall（CPU通知GPU使用相关的数据和渲染状态进行渲染）

渲染管线(流水线）中的应用阶段主要是CPU主导的阶段，它为渲染完成的最主要的工作就是提供后续的渲染数据，比如：顶点、法线、切线、纹理坐标、变换矩阵、材质属性等等。

在应用阶段中，我们主要就是按照Unity的规则进行游戏开发即可，需要注意的就是关于DrawCall的优化。

#### 关于DrawCall

DrawCall其实就是CPU命令GPU进行渲染的命令，为什么DrawCall多了会影响性能呢？

主要的性能瓶颈是CPU造成的，每次调用DrawCall之前，CPU需要向GPU发送很多内容，包括数据、状态命令等等。如果DrawCall过多，CPU就会把大量的时间花费在提交DrawCall上，造成CPU过载，让玩家感受到卡顿。

#### 如何减少DrawCall

使用批处理，可以有效的减少DrawCall，从而提升性能表现。

1. 合并网格（可以将静态物体合并网格）
2. 共用材质（在不同网格之间共用一种材质）
3. 合并图集（2D游戏和UI中，可以将多张图片合并为一张大图）

### 渲染管线——几何阶段

#### 必备知识：图元

在渲染管线中，图元是指几何数据的基本单元，它是构成几何体的最小可绘制的单元图元可以是点、线、三角形。

在渲染管线的几何阶段，顶点数据会被组合为图元。这些图元将在后续的光栅化阶段转换为像素，最终呈现在屏幕上。

#### 渲染管线在几何阶段主要做什么

渲染管线的几何阶段主要由GPU主导，因此我们无法拥有绝对的控制权，但是GPU为我们开放了部分控制权。几何阶段主要做的事情是**根据应用阶段输入的数据信息进行顶点坐标转换以及裁剪不可见图元**等工作。

#### 顶点着色器

顶点着色器处理来自应用阶段由CPU传递过来的顶点相关数据，输入进来的每一个顶点都会调用一次顶点着色器中的逻辑。

顶点着色器需要完成的工作主要有：
1. 坐标变换一顶点变换、法线变换、纹理坐标变换等。
2. 顶点属性处理一对顶点的其他属性进行处理，比如顶点颜色、透明度、切线向量等，可以用于实现顶点动画、着色、光照等效果。
3. 顶点插值—计算顶点属性的插值等等。

#### 曲面细分着色器、几何着色器

它们两对于对于我们来说是可选的着色器，并且他们需要硬件和驱动程序的支持才能使用。

#### 裁剪

裁剪阶段会自动的将不在视野内和部分在视野内的图元（点、线、三角形）进行裁剪，我们可以进行一些配置，但是一般我们不需要进行任何处理，渲染管线会自动帮助我们进行处理。

#### 屏幕映射

将输入的三维坐标系下的图元坐标转换到屏幕坐标系中。

#### 几何阶段为渲染准备了什么

在渲染管线（流水线）的几何阶段，最主要做的工作就是**对顶点进行处理，并进行坐标转换（将模型的顶点从其本地坐标转换到最终的屏幕坐标中），裁剪画面外的图元**。

对于我们来说，我们只要在顶点着色器中进行一些操作就可以带来不同的表现效果的体现，比如：水波纹、布料等等。

### 渲染管线——光栅化阶段

#### 必备知识：像素

像素是计算机图形学中的基本概念，它是组成图像的最小可控单位，具有位置和属性，用于表示图像中的颜色和其他信息。它是**二维图像中的一个点**，每个像素都占据屏幕上的一个固定位置。

比如我们常见的显示器分辨率为1920×1080，就表示宽度为1920个像素，高度为1080个像素。

#### 必备知识：片元

在渲染管线中，片元是指在光栅化阶段生成的像素或像素片段。

片元是渲染管线中进行像素级别操作和计算的基本单位，每个片元代表了屏幕上的一个像素，并且具有位置信息和与之相关的属性，比如：颜色、深度值、法线等等。

#### 渲染管线在光栅化阶段主要做什么

渲染管线的光栅化阶段同样由GPU主导，GPU为我们开放了部分控制权。

光栅化阶段主要做的事情是**根据几何阶段输入的信息计算每个图元覆盖哪些像素，以及为这些像素计算他们的颜色等等工作**。

#### 三角形设置

几何阶段输入到光栅化阶段的数据主要是三角形网格的顶点信息，我们得到的只是三角形网格每条边的两个端点信息。

如果想要得到整个三角形网格对像素的覆盖情况，就必须计算每条边上的像素坐标，为了能计算三角形边界像素的坐标信息，我们必须得到三角形边界的表示方式。

在三角形设置这个小阶段，GPU主要做的事情就是**计算三角形网格的表示数据**。

#### 三角形遍历

该阶段主要根据三角形设置中计算出的三角形网格数据，检查每个像素是否被一个三角形网格所覆盖。

如果覆盖的话就会生成一个片元（包含屏幕坐标、深度、法线等等信息），这个阶段也被成为扫描变换。

在三角形遍历这个小阶段GPU主要做的事情就是**根据三角形网格信息得到被它们覆盖的片元序列**。

#### 片元着色器

片元着色器主要完成对三角形遍历输入的片元序列中的每个片元（像素）的着色计算和属性处理片元着色器。

需要完成的工作主要有：
1. **光照计算**——计算片元的光照效果
2. **纹理映射**——根据片元在纹理中的位置，对纹理进行采样，将纹理颜色映射到片元上，实现表面贴图效果
3. **材质属性处理**——根据材质的属性，比如颜色、透明度、反射率等，计算片元的最终颜色和透明度
4. **阴影计算**——根据光源等信息，计算片元是否处于阴影中，影响其最终颜色等等

片元着色器是完全可编程的。

#### 逐片元操作

逐片元操作（输出合并阶段）主要完成对片元着色器输出数据（最终颜色、法线、纹理坐标、深度等）进行各种处理和计算逐片元操作。

主要完成的工作主要有：
1. 决定每个片元的可见性，比如深度测试、模板测试
2. 如果通过了所有测试，需要把片元的颜色值和已经存储在颜色缓冲区的颜色进行合并（混合）等等

逐片元操作是可配置的。

#### 光栅化阶段为渲染准备了什么

在渲染管线（流水线）的光栅化阶段，最主要做的工作就是**对片元（像素）进行最终处理，确定片元（像素）最终是否渲染到屏幕上，并且确定其的最终渲染的颜色效果**。

对于我们来说，我们只要在片元着色器中进行一些处理就可以带来不同的表现效果的体现比如：逼真的水面效果、火焰、黑白、模糊等等效果。

## Shader开发是什么

### Shader是什么

Shader的中文意思是着色器，是一种用于描述如何渲染图形和计算图形外观的程序。

主要用于控制图形的颜色、光照、纹理和其他视觉效果着色器通常由着色器语言编写，这些着色器语言提供了指令和语法用于编写描述光照、纹理映射、阴影、反射等图形外观的代码。

说人话：**Shader就是着色器，是用于编写图形表现效果的程序代码**。

Shader开发**本质就是对渲染管线（流水线）中上一阶段传递过来的数据进行自定义处理后再传递给下一阶段，通过自定义处理，让图形图像最终能够以我们想要的方式显示到屏幕上**。

### Shader和渲染管线的关系

渲染管线（流水线）的基本概念是将数据分阶段的变为屏幕图像的过程，而Shader开发就是针对其中某些阶段的自定义开发从而决定图形图像最终呈现到屏幕上的表现效果。

在Unity中，我们学习的Shader开发主要针对渲染管线中的两个小阶段：几何阶段中的**顶点着色器**和光栅化阶段中的**片元着色器**。

### 如何学习Shader开发

学习Shader开发，必须要学习的基本知识有：
1. 数学相关知识
2. 语法相关知识
3. 着色器开发相关知识

#### 数学相关知识

在渲染管线的几何阶段，我们要处理的核心工作之一就是坐标转换，我们要了解坐标转换的原理，就需要学习数学相关的知识，主要包含的内容有：**向量**相关知识和**线性代数**相关知识。

在顶点着色器中，我们就需要利用这些知识完成坐标转换等相关的工作。

#### 语法相关知识

我们已经知道我们需要在渲染管线的几何阶段中的顶点着色器和光栅化阶段中的片元着色器两个小阶段自定义处理数据来达到各种不同的表现效果，而想要实现自定义逻辑处理，我们就需要学习着色器开发的特定语言。

在Unity当中的Shader开发，我们需要学习Unity中的**ShaderLab语法**和**着色器开发的CG语言**等。

#### 着色器开发相关知识

渲染管线的本质是将数据最终呈现为屏幕图像，那为了让最终的图像效果更加的好，更加的符合需求，我们就必须学习一些效果处理的计算规范，比如：光照效果的颜色如何通过计算得到，纹理颜色应该如何从图片中获取，透明效果、阴影效果应该如何计算处理等等。

## 其他必备概念
### 计算机图形程序接口

计算机图形程序接口（Graphics API）对于游戏开发程序员来说是非常重要的知识，是学习图形学时必不可少的内容。

计算机图形程序接口是一套可编程的开放标准，不管做2D还是3D游戏都需要这部分的底层API支持。

它本质上是软件，并不是硬件，是前辈们提前为你写好的调用系统硬件（GPU）绘制图形的代码。我们甚至可以把它简单理解成是显卡厂商定义的一系列的底层的进行图形操作的加速API接口。

**由于目前各种游戏引擎的出现，即使你没有系统的学习过图形学相关知识，也能够独立的通过游戏引擎开发游戏**。但是你必须知道的是，**游戏引擎的一部分本质就是对图形程序接口的封装**，**游戏引擎通过图形程序接口**帮助我们**完成了图像渲染相关的工作**，我们只需要把工作重心放在游戏逻辑开发上了。

因此，计算机图形程序接口对于游戏开发来说，是非常重要的内容，即使你现在还没有时间，或者没有需求学习计算机图形学，你也必须对他们有一定的认识。

### Unity程序员必了解的图形程序接口

#### OpenGL（Open Graphics Library）

中文翻译过来是**开放图形库**，它定义了一个**跨平台、跨语言**的编程接口规格的专业图形程序接口，可以用于3D、2D图形渲染，是一个功能强大、调用方便的底层图形库。由于它跨平台、跨语言、出现时间早，因此它的应用极其广泛！

#### OpenGL ES（OpenGL for Embedded Systems）

中文翻译过来是**用于嵌入式系统的开放图形库**，它是**OpenGL的子级**，**主要针对手机、游戏主机**等嵌入式设备而设计，免授权费、跨平台、功能完善。

GLES2.0、GLES3.0 指的就是OpenGL ES这套标准，他们也是Android和IOS手机上常用的图形处理标准。

**Unity在移动平台进行图形渲染处理时，就包含了OpenGL ES方案。**

#### Vulkan

“下一代”开放的图形显示API，是与DX12能够匹敌的GPU API标准。它有一套最新的图形加速API接口，目标是提供更灵活和丰富的底层操作接口，以替代OpenGL 和 OpenGL ES接口，**可****以把Vulkan看做是OpenGL的升级版**，目前**新版本的Unity支持使用Vulkan方案**。

#### Directx（Direct eXtension）

中文翻译过来是直接拓展，简称DX。它是由**微软**公司创建的多媒体编程接口。它**不跨平台**，只针对微软的相关产品，被广泛使用于Windows操作系统、xBox游戏主机的图形应用程序开发中。

其中的D3D算是DX一部分，是对标OpenGL的图形程序接口。

#### WebGL（Web Graphics Library）

中文翻译过来是网页图形库，它是**针对Web端（**网页**）的3D绘图协议**，这个标准允许把JavaScript和OpenGL ES 2.0结合在一起，网页开发人员可以借助系统显卡在浏览器里流畅的展示3D场景和模型，可以在网页里进行3D图形开发。

#### Metal

中文翻译过来是金属，它是**苹果公司**为游戏开发者提供的图形技术，该技术能够为3D图像提高10倍渲染性能，但是它不支持跨平台，主要针对IOS、macOS苹果自家的操作系统，只有苹果手机、电脑能够使用。

### 对于我们的意义

了解这些图形程序接口的基本概念对我们有什么意义呢？

我们从他们的简单介绍中需要知道，他们主要支持的平台为

- **Windows电脑：DX、OpenGL、Vulkan**
- **苹果电脑：Metal、OpenGL、Vulkan**
- **安卓手机：OpenGL ES、Vulkan**
- **苹果手机：OpenGL ES、Vulkan、Metal**
- **网站网页：Web GL**

### 渲染管线(流水线）和图形接口程序的关系

图形接口程序（OpenGL、DX等）主要用于控制和管理渲染管线流程。

通过图形接口程序提供的API，我们就可以配置和操作渲染管线中的某些阶段设置输入数据、控制图形处理、应用各种渲染效果，最终实现图形渲染和呈现。

图形接口程序充当了开发者和图形硬件之间的中间层，将开发者的渲染命令和设置转化为硬件能够理解和执行的指令。

说人话：**图形接口程序（OpenGL、DX等）提供了对渲染管线（流水线）的控制和管理功能，它是开发者和硬件打交道的中间层**。

### Shader和图形接口程序的关系

Shader（着色器）是一种小型程序，用于自定义渲染数据的处理，从而决定最终的渲染效果。

图形接口程序（OpenGL、DX等）为Shader开发提供了各种APl，Shader开发需要针对不同的图形接口程序使用不同的Shader开发语言来调用相关APl。

图形接口程序会将Shader程序和渲染管线的各个阶段连接起来，它会把我们的数据和指令传递给硬件（GPU等），从而实现图形渲染的最终呈现

说人话：**Shader属于图形接口程序（OpenGL、DX等）的一部分**。

### 不同图形接口程序对Shader开发的影响

1. 使用的着色器语言不同
	- OpenGL: GLSL (OpenGL Shading Language)
	- DX: HLSL (High-Level Shading Language)
	- Metal: MSL (Metal Shading Language)
	- WebGL: GLSL ES (OpenGL ES Shading Language)
2. 坐标系原点不同
	- OpenGL、WebGL、Metal: 原点位于屏幕左下角
	- DX: 原点位于屏幕左上角角（注意：最新的DX12可以改为左下角原点）

# Shdaer必备基础

## 数学基础

### 线性代数

> 线性代数是数学的一个分支学科，它是一门研究向量和它们之间关系的数学学科。

我们可以把向量想象成有大小和方向的箭头，线性代数主要研究如何使用这些箭头进行加减乘除运算，以及它们之间的变换规则。

简而言之：线性代数是一门研究向量和变换的数学学科。

### 矩阵的基本概念

> 矩阵（Matrix）是线性代数中的一个核心概念和重要工具。

通过矩阵，我们可以方便的进行向量的相关计算，也可以更好的理解和解决线性代数中的各种问题。

简而言之：矩阵是一种用来表示和处理数据的数学工具它可以帮助我们有效的管理和计算大量的数据

#### 矩阵在数学中的表示

矩阵可以通过方括号内的数值表格来表示，比如：
$$
\left [
\begin{matrix}
2 & 3 & 4 \\
3 & 4 & 5 \\
7 & 8 & 3
\end{matrix}
\right ]
$$

矩阵是由m×n（m和n大于0）个标量（只有大小，没有方向的量，可以理解为单个数值）组成的，可以通过方括号内的数值表格来表示。

#### 矩阵在程序中的表示

在程序中存储矩阵结构的方式有很多选择，最常见的有：
1. 数组（一维或二维）
2. 嵌套列表（两个List）
3. 开发工具提供的类或结构体（Unity中的Matrix4x4、Matrix3x2）

#### 为什么要学习矩阵

在Shader开发中的很多数学计算都需要利用矩阵来完成，比如:坐标系转换、投影计算、光照计算、纹理映射等等。

所以学习矩阵的目的，就是为了能在Shader开发中利用其进行相关数学计算。

### 矩阵乘法

#### 矩阵和标量的乘法

$$
kM = Mk = k\left [
\begin{matrix}
m11 & m12 & m13 \\
m21 & m22 & m23 \\
m31 & m32 & m33
\end{matrix}
\right ]
=\left [
\begin{matrix}
km11 & km12 & km13 \\
km21 & km22 & km23 \\
km31 & km32 & km33
\end{matrix}
\right ]
$$

#### 矩阵和矩阵的乘法

1. 首先判断两个矩阵是否能够相乘：左列和右行要相等。
2. 相乘得到的矩阵结构：左行右列。
3. 标量相乘的规则：左行乘右列再相加。

> 举例

$$
a = \left [
\begin{matrix}
3 & 6 & 9
\end{matrix}
\right ]
, 
b = \left [
\begin{matrix}
1 & 2 \\
3 & 4 \\
5 & 6
\end{matrix}
\right ]
$$
$$
ab = \left[
\begin{matrix}
3*1+6*3+9*5 & 3*2+6*4+9*6
\end{matrix}
\right]
= [66 \quad 64]
$$

> 注意事项

1. 矩阵乘法不满足交换律：$AB \neq BA$
2. 矩阵乘法满足结合律：$(AB)C=A(BC)$

### 特殊矩阵

#### 方块矩阵

方块矩阵简称为方阵，其行数和列数相同。

#### 对角矩阵

对角矩阵是一种特殊的方阵，只有主对角线有值，其余元素均为0。

#### 单位矩阵

单位矩阵是一种特殊的对角矩阵，主对角线上的元素均为1。

#### 数量矩阵

数量矩阵是一种特殊的对角矩阵，主对角线上的元素值相同。

#### 转置矩阵

转置矩阵是将原始矩阵的行和列互换得到的新矩阵，假设有矩阵$M$，则其转置矩阵记为$M^T$。

$$
\left[
\begin{matrix}
1 & 2 \\
5 & 6 \\
9 & 8
\end{matrix}
\right]^T
=
\left[
\begin{matrix}
1 & 5 & 9 \\
2 & 6 & 8
\end{matrix}
\right]
$$

> 性质

1. $(M^T)^T=M$
2. $(AB)^T=B^T A^T$

#### 逆矩阵

逆矩阵必须是一个方阵，并且不是所有矩阵都有逆矩阵。

假设有一个方阵$M$，其逆矩阵记为$M^{-1}$，$MM^{-1}=M^{-1}M=I(单位矩阵)$。

如果一个矩阵存在逆矩阵，则称该矩阵是可逆的（非奇异的），否则称为不可逆的（奇异的）。

> 计算逆矩阵

1. 确定矩阵为方阵（即行列数相等）
2. 计算矩阵的行列式（若行列式为零，则该矩阵没有逆矩阵）
3. 计算矩阵的代数余子式矩阵
4. 计算标准伴随矩阵（转置代数余子式矩阵）
5. 计算逆矩阵（标准伴随矩阵/行列式）

> 行列式的计算方式

以3×3的行列式计算为例：

$$
\left|
\begin{matrix}
m_{11} & m_{12} & m_{13} \\ 
m_{21} & m_{22} & m_{23} \\ 
m_{31} & m_{32} & m_{33} \\ 
\end{matrix}
\right|
=
m_{11}m_{22}m_{33} + m_{12}m_{23}m_{31} + m_{13}m_{21}m_{32}
- m_{31}m_{22}m_{13} - m_{32}m_{23}m_{11} - m_{33}m_{21}m_{12}
$$

> 代数余子式矩阵的计算方式

假设矩阵$M$的代数余子式矩阵为矩阵$C$。

$$
M=\left[
\begin{matrix}
m_{11} & m_{12} & m_{13} \\ 
m_{21} & m_{22} & m_{23} \\ 
m_{31} & m_{32} & m_{33} \\ 
\end{matrix}
\right]

C=\left[
\begin{matrix}
c_{11} & c_{12} & c_{13} \\ 
c_{21} & c_{22} & c_{23} \\ 
c_{31} & c_{32} & c_{33} \\ 
\end{matrix}
\right]
$$

标量的代数余子式计算规则：$C_{ij}=(-1)^{i+j}*去掉第i行、第j列组成的矩阵行列式$

> 标准伴随矩阵的计算方式

标准伴随矩阵是原矩阵的代数余子式矩阵的转置矩阵。

> 逆矩阵的计算方式

逆矩阵=标准伴随矩阵/行列式

> 逆矩阵的重要性质

1. 逆矩阵的逆矩阵是原矩阵本身：$（M^{-1})^{-1}=M$
2. 矩阵乘以自己的逆矩阵的结果是单位矩阵：$MM^{-1}=M^{-1}M=I$
3. 单位矩阵的逆矩阵是它本身：$I^{-1}=I$
4. 转置矩阵的逆矩阵是逆矩阵的转置：$(M^T)^{-1}=(M^{-1})^T$
5. 矩阵串接相乘后的逆矩阵等于反向串接各个矩阵的逆矩阵相乘：$(AB)^{-1}=B^{-1}A^{-1}$
6. 逆矩阵可以计算矩阵变换的反向变换（M为矩阵，v为一个矢量）：$M{-1}(Mv)=(M^{-1}M)v=Iv=v$

#### 正交矩阵

正交矩阵是一种特殊的方阵，正交的意思是垂直。

一个方阵和它的转置矩阵相乘为单位矩阵，那么它就是正交矩阵，$MM^T=M^TM=I$。

根据逆矩阵的一个重要性质：$MM^{-1}=M^{-1}M=I$，可以推导出正交矩阵的逆矩阵等于其转置矩阵，即$M^T=M^{-1}$。

如果一个矩阵是正交矩阵，那么其转置矩阵也是正交矩阵。

$$
M=\left[
\begin{matrix}
- & Ⅰ & - \\
- & Ⅱ & - \\
- & Ⅲ & -
\end{matrix}
\right], \quad
M^T=\left[
\begin{matrix}
- & - & - \\
Ⅰ & Ⅱ & Ⅲ \\
- & - & -
\end{matrix}
\right]
$$

$$
MM^T=\left[
\begin{matrix}
Ⅰ*Ⅰ & Ⅰ*Ⅱ & Ⅰ*Ⅲ \\
Ⅱ*Ⅰ & Ⅱ*Ⅱ & Ⅱ*Ⅲ \\
Ⅲ*Ⅰ & Ⅲ*Ⅱ & Ⅲ*Ⅲ 
\end{matrix}
\right]
=
\left[
\begin{matrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{matrix}
\right]
$$

得出结论：

1. 正交矩阵的每一行都是单位向量（自己点乘自己的结果为1）。
2. 正交矩阵的每一行都相互垂直（彼此点乘的结果为0）。
3. 以上结论对正交矩阵的每一列也适用，因为正交矩阵的转置也是正交矩阵。

> 如何判断正交矩阵

1. 若满足$MM^T=M^TM=I$，则为正交矩阵。
2. 判断矩阵的每一行（列）是否为单位向量，以及矩阵的行（列）向量是否彼此垂直，满足即为正交矩阵。

> 正交矩阵的重要性质

如果矩阵$M$为正交矩阵，那么：

1. $M^T$也为正交矩阵
2. $M^T=M^{-1}$
3. 正交矩阵的行列式为1或-1
4. 正交矩阵的各行各列均为单位向量且彼此正交（垂直）

#### 列矩阵和行矩阵

在Unity Shader开发中，经常会对向量进行矩阵运算。在三维坐标系中，向量一般都是$(x, y, z)$表示的，由于要进行矩阵计算，我们就需要把向量用矩阵表示。

三维向量的矩阵表示有两种：

1. 列矩阵：$\left[\begin{matrix}x\\ y\\ z\end{matrix}\right]$
2. 行矩阵：$\left[\begin{matrix}x & y & z\end{matrix}\right]$

在进行向量的矩阵运算时，把它作为行矩阵和列矩阵得到的结果是不同的。

假设有矩阵$M=\left[\begin{matrix}m_{11} & m_{12} & m_{13} \\ m_{21} & m_{22} & m_{23} \\ m_{31} & m_{32} & m_{33} \\ \end{matrix}\right]$，一个向量以列或行矩阵的形式和它计算时，计算顺序会不同，结果也会不同。

和3×3矩阵进行乘法运算时，列矩阵必须放在右侧才能计算，而行矩阵必须放在左侧。

> 列矩阵和行矩阵在Unity中的使用规则

在标准的线性代数中，矩阵和向量的乘法是按列进行的，所以在Unity的Shader开发中，也遵循这种规则。

使用列矩阵的结果是，我们的阅读顺序是从右往左的。

假设向量为列矩阵$v$，$A,B,C$分别为3个变换矩阵，$CBAv=C(B(Av))$，即先对向量$v$使用$A$进行变换，再依次使用$B$和$C$进行变化。

> 注意

如果想把向量作为行矩阵处理，也是可以的，为了使结果相同，行矩阵必须乘以变换矩阵的转置矩阵。

即$vA^TB^TC^T=CBAv=C(B(Av))$。

### 矩阵的几何意义

点和向量能在图像中画出来，那么矩阵可以吗？

答案是肯定的，矩阵的可视化结果是：变换。

在游戏开发中，如果你看到了一个矩阵，那么基本上你可以认为你看到的是一个变换，这些变换一般包含：平移、旋转、缩放。

比如，我们想要将一个点、一个向量进行一种变换，那么就可以利用矩阵来进行计算，从而达到变换的目的。

> 变换的种类

在数学中常用的几何变换有两种：线性变换和仿射变换，它们的主要区别在于是否保持直线的平行性和原点位置。

- 线性变换：保持直线的平行性和原点位置不变，比如缩放、旋转等操作，简而言之就是只对向量进行旋转、缩放等操作，而不影响方向和原点的位置。
- 仿射变换：由一个线性变换和一个平移组成，比如缩放后、旋转后再平移，简而言之就是缩放后、旋转后平移，会改变原点的位置。

### 齐次坐标

齐次坐标是一种在计算机图形学中常用的表示坐标的方式，它是通过引入一个额外的维度来扩展传统的笛卡尔坐标系，就是将一个原本是n维的向量或矩阵用n+1维来表示，让我们可以更方便地进行几何变换和矩阵运算。

> 举例

三维空间中有一个向量或点$(x,y,z)$，它对应的齐次坐标就是给加一维，变成$(x,y,z,w)$，其中$w$值的改变可以让它具有不同的含义。

> 为什么要用齐次坐标进行矩阵运算

1. 使用齐次坐标可以明确地区分向量和点，当$w=1$时代表一个点，$w=0$时代表一个向量。
2. 3×3矩阵不能直接表示平移变换，只能表示线性变换。

3×3矩阵一般称为线性矩阵，主要处理线性变换（旋转、缩放等）。

4×4矩阵一般称为仿射矩阵，主要处理仿射变换（线性变换+平移变换）。

### 平移矩阵

#### 基础变换矩阵的构成规则

4×4矩阵的基本构成规则：

$$
\left[
\begin{matrix}
M_{11} & M_{12} & M_{13} & t_{1} \\
M_{21} & M_{22} & M_{23} & t_{2} \\
M_{31} & M_{32} & M_{33} & t_{3} \\
0 & 0 & 0 & 1
\end{matrix}
\right]
=> 
\left[
\begin{matrix}
M^{3×3} & t^{3×1} \\
0^{1×3} & 1
\end{matrix}
\right]
$$

- $M^{3×3}$部分表示旋转和缩放变换
- $t^{3×1}$部分表示平移
- $0^{1×3}$始终为零矩阵
- 右下角元素始终为1

#### 平移矩阵的构成

$M^{3×3}$部分为3×3单位矩阵，$t^{3×1}$部分表示$x,y,z$平移多少单位。

$$
\left[
\begin{matrix}
M^{3×3} & t^{3×1} \\
0^{1×3} & 1
\end{matrix}
\right]
=>
\left[
\begin{matrix}
1 & 0 & 0 & t_x \\
0 & 1 & 0 & t_y \\
0 & 0 & 1 & t_z \\
0 & 0 & 0 & 1
\end{matrix}
\right]
$$

#### 平移矩阵的计算

> 与点之间的计算

$$
\left[
\begin{matrix}
1 & 0 & 0 & t_x \\
0 & 1 & 0 & t_y \\
0 & 0 & 1 & t_z \\
0 & 0 & 0 & 1
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y \\ z \\ 1
\end{matrix}
\right]
= \left[
\begin{matrix}
x+t_x \\ y+t_y \\ z+t_z \\ 1
\end{matrix}
\right]
$$

从该计算便可以看出为什么3×3矩阵无法表示平移，而是需要使用齐次坐标4×4的矩阵。

在几何图像中的效果是，将点$(x,y,z)$在3D空间中平移了$(t_x, t_y, t_z)$个单位。

> 与向量之间的计算

$$
\left[
\begin{matrix}
1 & 0 & 0 & t_x \\
0 & 1 & 0 & t_y \\
0 & 0 & 1 & t_z \\
0 & 0 & 0 & 1
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y \\ z \\ 0
\end{matrix}
\right]
= \left[
\begin{matrix}
x\\ y\\ z\\ 0
\end{matrix}
\right]
$$

从该计算可以发现，向量的平移结果是不会有任何变化的。

#### 平移矩阵的逆矩阵

平移矩阵不是正交矩阵，因为其逆矩阵不能通过$M^T=M^{-1}$得出。

经过计算，平移矩阵的逆矩阵为$\left[\begin{matrix} 1&0&0&-t_x \\ 0&1&0&-t_y \\ 0&0&1&-t_z \\ 0&0&0&1 \end{matrix}\right]$。

### 旋转矩阵

#### 旋转矩阵的构成

旋转操作需要指定一个旋转轴（不一定是空间中的坐标轴），即绕x轴、y轴或z轴进行旋转。

绕x轴旋转$\beta$度的旋转矩阵：

$$
\left[
\begin{matrix}
1 & 0 & 0 & 0 \\
0 & \cos\beta & -\sin\beta & 0 \\
0 & \sin\beta & \cos\beta & 0 \\
0 & 0 & 0 & 1
\end{matrix}
\right]
$$

绕y轴旋转$\beta$度的旋转矩阵：

$$
\left[
\begin{matrix}
\cos\beta & 0 & \sin\beta & 0 \\
0 & 1 & 0 & 0 \\
-\sin\beta & 0 & \cos\beta & 0 \\
0 & 0 & 0 & 1
\end{matrix}
\right]
$$

绕z轴旋转$\beta$度的旋转矩阵：

$$
\left[
\begin{matrix}
\cos\beta & -\sin\beta & 0 & 0 \\
\sin\beta & \cos\beta & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{matrix}
\right]
$$

#### 旋转矩阵的计算

点和向量与旋转矩阵进行计算都会发生改变，其几何意义就是点或向量围绕某一个轴进行旋转，得到一个新的点或向量。

#### 旋转矩阵的逆矩阵

因为旋转矩阵是正交矩阵，所以旋转矩阵的逆矩阵就是其转置矩阵。

### 缩放矩阵

#### 缩放矩阵的构成

缩放矩阵主要是对点或向量进行缩放操作，用于改变向量或点在各个坐标轴上的尺度，使其在各个方向上变大或变小，在三维空间中，主要有x、y、z轴的缩放因子构成。

$$
\left[
\begin{matrix}
kx & 0 & 0 & 0 \\
0 & ky & 0 & 0 \\
0 & 0 & kz & 0 \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

当$kx=ky=kz$时，称为统一缩放，否则为不统一缩放。

#### 缩放矩阵的计算

缩放矩阵的几何意义：

- 对点的缩放（一般是构成模型的顶点），相当于是在缩放模型的大小。
- 对向量的缩放，统一缩放时只会改变向量的大小，不会改变方向；非统一缩放时不仅会改变大小，可能还会改变方向。

#### 缩放矩阵的逆矩阵

缩放矩阵通常不是正交矩阵，通过计算得出缩放矩阵的逆矩阵为：

$$
\left[
\begin{matrix}
\frac 1{kx} & 0 & 0 & 0 \\
0 & \frac 1{ky} & 0 & 0 \\
0 & 0 & \frac 1{kz} & 0 \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

### 复合运算

复合运算就是在计算矩阵变换时，把平移、旋转、缩放等计算组合起来，通过结合多矩阵的乘法，来形成一个复杂的变换过程。

例如将一个模型先缩放到2倍大小，再绕y轴旋转60°，最后再向x轴平移5个单位。

这种复合变换过程，可以通过矩阵的串联来实现 ，即$P_新=M_{平移}M_{旋转}M_{缩放}P_老$，从右往左计算。

> 计算顺序对结果的影响

在进行复合运算时，变换的结果依赖于变换的顺序。

因为矩阵乘法不满足交换律，不同的计算顺序的结果是不一样的。

> Unity中需要遵守的规则

1. 在进行平移、旋转、缩放的复合运算时，绝大多数情况下的变换顺序为：缩放->旋转->平移。
2. 在进行x轴、y轴、z轴旋转的复合运算时，绝大多数情况下的变换顺序为：z->x->y。

### 坐标空间变换

坐标空间是一个用于描述和定位物体位置的数学概念，一般是有一个基础参照物（原点）和轴线（相互垂直）组成。

#### 为什么有很多不同的坐标空间

之所以存在那么多不同的坐标空间，主要是因为不同的问题需要不同的坐标系来描述和解决特定的空间问题，帮助我们更方便地完成需求。

#### 坐标空间的变换

在Shader开发中，为了方便我们制作模型、使用模型、渲染模型，也存在很多不同的坐标空间，比如模型空间、世界空间、观察空间、裁剪空间、屏幕空间。

我们这里的坐标空间变换主要是指，在渲染管线中，我们需要将坐标数据，在这几种空间总进行变换计算。

例如，在设计模型时，使用的是模型空间（所有的顶点、法线等数据都是基于模型空间坐标系的），当我们将模型导入到Unity后，最后能够在屏幕上显示，这里面就经历了我们看不到的坐标空间变换：模型空间->世界空间->观察空间->裁剪空间->屏幕空间。

#### 坐标空间的变换规则

> 坐标空间的组成

想要定义一个坐标空间，必须具备以下两点：

1. 坐标原点位置
2. 3个坐标轴的方向

> 坐标空间之间的关系

在Unity中，世界坐标空间相当于基础坐标空间，其他大部分的坐标空间都是世界坐标空间的子坐标空间。

这些子坐标空间的原点和轴向的相关表示数据，都是基于世界坐标空间的。

因此，Unity中的坐标空间变换实际上就是父空间和子空间之间对点或向量进行变换。

> 坐标空间的变换矩阵

目前的平移、旋转和缩放矩阵是无法完成坐标空间之间的变换的，所以我们必须对坐标空间的变换矩阵进行分析，才能得到我们想要的变换矩阵。

假设一个父坐标空间为F，子坐标空间为S。

已知S坐标空间的原点位置和3个单位坐标轴（基于F坐标空间的数据表达）对于坐标空间转换我们一般会有以下两种需求：

1. 把子坐标空间S下的点或向量$A_s$转换到父坐标空间F中为$A_f$
2. 把父坐标空间F下的点或向量$B_f$转换到子坐标空间S中为$B_s$

如果用矩阵来表示的话：

$$
A_f=M_{s-f}A_s
$$
$$
B_s=M_{f-s}B_f
$$

$M_{f-s}$和$M_{s-f}$互为逆矩阵，也就是说只要知道其中一个，就可以通过求逆矩阵的方式得到另一个矩阵。

 根据推导得出$M_{s-f}$子坐标空间到父坐标空间的变换矩阵为：

$$
\left[
\begin{matrix}
| & | & | & | \\
X_s & Y_s & Z_s & O_s \\
| & | & | & | \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

前三列分别是子坐标空间相对父坐标空间的x、y、z轴的方向向量，第四列是子坐标空间相对父坐标空间的原点。

### 模型空间变换

#### 模型空间的意义

模型空间也成为对象空间或局部空间，一般指3D模型的局部坐标系，每个模型都有自己独立的坐标空间。

模型空间的主要意义是方便建模，模型的顶点等数据都是基于模型空间表达的。

注意：在Unity中当模型移动或旋转时，模型空间坐标系也会随着变换，因为此时的模型坐标空间是世界坐标空间的子空间。

#### 模型空间中的注意事项

在模型空间中，一般有上、下、左、右、前、后，六种方向概念。Unity使用的是左手坐标系，因此模型空间的x、y、z轴，对应的是模型的右、上、前三个方向。

需要注意的是，在不同的软件中，模型空间中的xyz不一定是上面这种关系，因此在导出模型时需要修改相关设置，让导出的模型满足Unity的规范。

#### 模型空间变换指什么

模型空间变换指的主要是将模型空间中的点或向量通过矩阵乘法计算，变换为相对于世界坐标空间下的数据。

在渲染管线的几何阶段和光栅化阶段中，我们需要将顶点等数据进行相关变换，让其最终能够显示在屏幕上，而模型空间变换就是其中一个重要的变换步骤，就是将模型空间下的点和向量数据转换到世界空间下进行表示。

#### 如何进行模型空间变换

在进行复合运算时，一定要遵守先缩放、后旋转、在平移的规则，可以得到等式：$相对世界坐标系的位置 = 平移矩阵 * 旋转矩阵 * 缩放矩阵 * 模型空间下的点或向量$。

当存在多层模型父子关系时，直接一层层往上计算接口，或者直接使用transform当中的position、rotation、lossyScale变量进行计算。

> 利用坐标空间变换规则进行变换

当使用坐标空间规则进行计算时，如果存在缩放，只需要用x、y、z轴向的单位向量乘以对应轴的缩放因子即可。

### 观察空间变换

#### 观察空间的意义

观察空间也称为摄像机空间，观察空间可以认为是一个特殊的模型空间，这里的模型指的是场景中的摄像机。

摄像机是一个特殊的模型，它不可见，但是它可以决定我们在屏幕上看到的内容，因为此我们将摄像机的模型空间单独提出来讨论和学习，并将它称为观察空间。

观察空间的主要意义是摄像机决定了渲染的视角和视野。

#### 观察空间中的注意事项

在Unity中的模型空间遵循左右坐标系原则，但是在观察空间中遵守的是右手坐标系原则，因此它的坐标轴方向有所不同，观察空间中的x、y、z轴的正方向分别对应摄像机的右、上、后方。

造成这个情况的主要原因是：在OpenGL中，为了统一处理场景中物体的渲染和投影，通常使用右手坐标系，为了方便之后的数据处理，因此在Unity中的观察空间也遵循右手坐标系。

#### 观察空间变换指什么

观察空间变换指的主要是将模型空间中的点或向量从世界空间中变换到观察空间中，它是顶点变换的第二步，就是将数据从世界空间->观察空间进行变换。

观察空间变换也可以称为观察变换。

#### 如何进行观察空间变换

1. 利用坐标空间变换规则
2. 逆向变换观察空间：让观察空间和世界坐标空间重合，重合时观察空间下的点或向量的数据表达和世界空间下的数据表达式相同。

> 方法一：坐标空间变换规则

1. 利用Unity中的Transform相关API，得到观察空间的轴向和原点位置（相对世界坐标系）
2. 得到子到父的变换矩阵，再对该矩阵进行逆矩阵计算，得到父到子的变换矩阵。
3. 用父到子的变换矩阵和目标点进行矩阵运算。

> 方法二：变换观察空间，让观察空间和世界坐标空间重合

1. 观察摄像机的Transform
2. 根据Transform信息获得摄像机逆向变换的矩阵，让其和世界坐标系重合
3. 由于观察空间的z轴正方向是后方，所以需要对z分量进行取反
4. 用该变换矩阵和点进行乘法运算得到最终结果

### 裁剪空间变换

#### 齐次裁剪空间

裁剪空间也被称为齐次裁剪空间，它是一个非常特殊的坐标空间，通过将摄像机的视锥体投影到一个规范化的立方体而转换来。

在计算机图形学中用于在图形渲染过程中进行裁剪和投影，坐标范围从(-1,-1,-1)到(1,1,1,1)，超出这个范围的坐标在渲染时会被裁剪掉，只会保留范围内的坐标。

> 视锥体

摄像机的视锥体是在三维空间中表示摄像机可见区域的虚拟体积，类似一个六面体的形状，根据摄像机的属性和投影方式而定。

视锥体定义了摄像机在场景中能够看到的物体区域，超出这个区域的物体将在渲染时被裁剪掉，从而提高渲染性能。

视锥体主要包含的重要部分：

- 远近裁剪平面
- 左右上下裁剪平面

透视投影中，视锥体类似一个金字塔形状，远裁剪面比近裁剪面大，所以产生透视效果。

正交投影中，视锥体类似长方体的形状，远近裁剪面一样大，不会产生透视效果。

我们希望根据视锥体围成的区域对顶点等数据进行裁剪，超出视锥体这个范围的坐标在渲染时会被裁剪掉，只保留视锥体范围内的坐标。

但是如果直接使用视锥体定义的空间来进行裁剪，那不同的视锥体就需要不同的处理过程，而且对于透视投影的视锥体来说，判断顶点是否在范围内相对比较麻烦。

因为，我们希望用更通用、便捷的方式来进行裁剪工作，就需要将观察空间中的数据转换到齐次裁剪空间中。

#### 摄像机视锥体的投影方式

不管是透视还是正交投影摄像机，视锥体中看到的物体最终都会被投影到近裁剪面上来进行显示。

- 透视投影摄像机：视锥体内顶点和原点连接，在近裁剪面的交点为投影点。
	![[Pasted image 20240617112034.png]]
- 正交投影摄像机：视锥体内顶点向近裁剪面做左右面平行线，在近裁剪面交点为投影点。
	![[Pasted image 20240617112058.png]]

#### 相似三角形的重要性质

三个角分别相等，三边成比例的两个三角形叫做相似三角形。

1. 相似三角形的对应角相等。
2. 相似三角形的对应边成比例，比值k称为相似比。

#### 正交投影

本节的目标是得到将摄像机视锥体的正交投影空间转换到齐次坐标裁剪空间的变换矩阵。

分为以下两步来完成：

1. 将视锥体中心位移到观察空间原点中心。
2. 将长方体视锥体的xyz坐标范围映射到(-1,1)长宽高为2的正方体中。

> 正交投影变换矩阵

已知远近裁剪面离摄像机的距离为Near和Far，而观察空间中z方向是摄像机后方，所以视锥体中心的z坐标为$\frac {(-Near)+(-Far)} {2}$，于是得到第一步的平移矩阵如下。

$$
\left[
\begin{matrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & \frac {Near+Far} {2} \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

可以得到观察空间中的xyz和齐次坐标系中xyz的关系如下图。

![[Pasted image 20240617114155.png]]

根据这些关系图，可以得到如下公式。

$$
\begin{array}\
X_齐=\frac 2 {2\times Aspect\times Size} \times X_观 \\
Y_齐=\frac 2 {2\times Size}\times Y_观 \\
Z_齐=-\frac 2 {Far-Near}\times Z_观
\end{array}
$$

可以看出，这一步的变换其实就是一个缩放变换，因此我们可以根据上面的公式得到一个缩放矩阵。

$$
\left[
\begin{matrix}
\frac {1} {Aspect\times Size} & 0 & 0 & 0 \\
0 & \frac 1 {Size} & 0 & 0 \\
0 & 0 & -\frac 2 {Far-Near}  & 0 \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

现在已经得到了两步对应的平移矩阵和缩放矩阵，将它们进行乘法运算后，就可以得到将摄像机视锥体的正交投影空间转换到齐次坐标裁剪空间的变换矩阵。

$$
\left[
\begin{matrix}
\frac {1} {Aspect\times Size} & 0 & 0 & 0 \\
0 & \frac 1 {Size} & 0 & 0 \\
0 & 0 & -\frac 2 {Far-Near} & -\frac {Far+Near} {Far-Near} \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

#### 透视投影

已知近裁剪面和远裁剪面离摄像机的距离分别为Near和Far，以及视锥体开口的角度为FOV。

![[Pasted image 20240617160832.png]]

可得
$$
\begin{array} \
近裁剪面高=2 \times Near\times \tan {\frac {FOV} 2} \\
远裁剪面高=2 \times Far\times \tan {\frac {FOV} 2}
\end{array}
$$

对于裁剪面的宽，只需要乘以宽高比即可得到。

> 透视投影变换矩阵

分为以下三步来完成：

1. 将透视视锥体变成一个长方体。
2. 将视锥体中心位移到观察空间原点中心。
3. 将长方体视锥体的xyz坐标范围映射到(-1,1)长宽高为2的正方体中。

![[Pasted image 20240617161913.png]]

要实现从左到右的变换，需要做以下三点：

1. 近裁剪面上的所有点保持不变
2. 远裁剪面的z值不变，远裁剪面的中心点不变
3. 远裁剪面宽高映射成近裁剪面的宽高

对于近裁剪面上的点$(x,y,Near,1)$，满足$变换矩阵M\times\left[\begin{smallmatrix}x \\ y \\ -Near \\ 1\end{smallmatrix}\right]=\left[\begin{smallmatrix}x \\ y \\ -Near \\ 1\end{smallmatrix}\right]$。

对于远裁剪面上的点$(0,0,Far,1)$，满足$变换矩阵M\times\left[\begin{smallmatrix}0 \\ 0 \\ -Far \\ 1\end{smallmatrix}\right]=\left[\begin{smallmatrix}0 \\ 0 \\ -Far \\ 1\end{smallmatrix}\right]$。

![[Pasted image 20240617164645.png]]

对于x方向上点的映射，满足$X'^=X\frac{-Near}Z$。

对于y方向上点的映射，满足$Y'^=Y\frac{-Near}Z$。

可见，视锥体内所有点的x、y坐标，都经过了同样的缩放，缩放因子为$\frac{-Near}{Z}$，满足以下变换矩阵。

$$
变换矩阵M\times
\left[
\begin{matrix}
x \\ y \\ z \\ 1
\end{matrix}
\right]
=
\left[
\begin{matrix}
x\frac{-Near}Z \\ y\frac{-Near}Z \\ 未知 \\ 1
\end{matrix}
\right]
$$
> 补充知识点：四维齐次坐标中乘以或除以一个非零的数（向量），所映射的三维坐标始终是同一个坐标。

首先对第三步得到的变换矩阵乘以-z，将负号和Z消掉，即$\left[\begin{matrix}xNear \\ yNear \\ 未知 \\ -z\end{matrix}\right]$。

然后可以得到

$$
变换矩阵M=
\left[\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
? & ? & ？ & ？ \\
0 & 0 & -1 & 0 \\
\end{matrix}\right]
$$

于是，现在我们只需要推导出该变换矩阵第三行的构成就可以了，可以假设第三行中前两个位置都为0（因为z和x、y无关），后两个位置分别为a和b。

$$
变换矩阵M=
\left[\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
0 & 0 & a & b \\
0 & 0 & -1 & 0 \\
\end{matrix}\right]
$$

接着可以根据$变换矩阵M\times\left[\begin{smallmatrix}x \\ y \\ -Near \\ 1\end{smallmatrix}\right]=\left[\begin{smallmatrix}x \\ y \\ -Near \\ 1\end{smallmatrix}\right]$和$变换矩阵M\times\left[\begin{smallmatrix}0 \\ 0 \\ -Far \\ 1\end{smallmatrix}\right]=\left[\begin{smallmatrix}0 \\ 0 \\ -Far \\ 1\end{smallmatrix}\right]$进一步推导。

对于第一条式子，可以让x、y等于0，相当于近裁剪面的中心点，将变换矩阵M代入。

$$
\left[\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
0 & 0 & a & b \\
0 & 0 & -1 & 0 \\
\end{matrix}\right]
\times\left[\begin{matrix}0 \\ 0 \\ -Near \\ 1\end{matrix}\right]
=\left[\begin{matrix}0 \\ 0 \\ -Near \\ 1\end{matrix}\right]\times Near
=\left[\begin{matrix}0 \\ 0 \\ -Near^2 \\ Near\end{matrix}\right]
$$

对于第二条式子，直接将变换矩阵M代入即可。

$$
\left[\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
0 & 0 & a & b \\
0 & 0 & -1 & 0 \\
\end{matrix}\right]
\times\left[\begin{matrix}0 \\ 0 \\ -Far \\ 1\end{matrix}\right]
=\left[\begin{matrix}0 \\ 0 \\ -Far \\ 1\end{matrix}\right]\times Near
=\left[\begin{matrix}0 \\ 0 \\ -Far^2 \\ Far\end{matrix}\right]
$$

于是，可以得到下面两条等式。

$$
\left \{
\begin{align}
-aNear + b = -Near^2 \\
-aFar + b = -Far^2
\end{align}
\right.
$$

通过解方程可得$a=Far+Near, b=Far\times Near$。

因此第一步变换矩阵的结果是：

$$
M=\left[
\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
0 & 0 & Far+Near & Far\times Near \\
0 & 0 & -1 & 0 \\
\end{matrix}
\right]
$$

第二步平移矩阵和正交投影中的平移矩阵相同：

$$
\left[
\begin{matrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & \frac {Near+Far} {2} \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

第三步缩放矩阵也和正交投影中的类似：

$$
\left[
\begin{matrix}
\frac {1} {Aspect\times Near\times \tan\frac{FOV}2} & 0 & 0 & 0 \\
0 & \frac 1 {Near\times \tan\frac{FOV}2} & 0 & 0 \\
0 & 0 & -\frac 2 {Far-Near} & 0 \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]
$$

将三个矩阵相乘的结果就是将摄像机视锥体的透视投影空间转换到齐次坐标裁剪空间时的变换矩阵。

$$
\left[
\begin{matrix}
\frac {1} {Aspect\times Near\times \tan\frac{FOV}2} & 0 & 0 & 0 \\
0 & \frac 1 {Near\times \tan\frac{FOV}2} & 0 & 0 \\
0 & 0 & -\frac 2 {Far-Near} & 0 \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]\times
\left[
\begin{matrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & \frac {Near+Far} {2} \\
0 & 0 & 0 & 1 \\
\end{matrix}
\right]\times
\left[
\begin{matrix}
Near & 0 & 0 & 0 \\
0 & Near & 0 & 0 \\
0 & 0 & Far+Near & Far\times Near \\
0 & 0 & -1 & 0 \\
\end{matrix}
\right]=
\left[
\begin{matrix}
\frac {1} {Aspect\times \tan\frac{FOV}2} & 0 & 0 & 0 \\
0 & \frac 1 {\tan\frac{FOV}2} & 0 & 0 \\
0 & 0 & -\frac {Far+Near} {Far-Near} & -\frac {2Far\times Near} {Far-Near} \\
0 & 0 & -1 & 0 \\
\end{matrix}
\right]
$$

#### 裁剪空间变换的意义

裁剪空间变换的意义是让我们以一种更通用、方便的方式来进行裁剪工作。

因为如果直接使用视锥体定义的空间来进行裁剪，那不同的视锥体就需要不同的处理过程，对于透视投影的视锥体来说，判断顶点是否在其范围内相对较麻烦。

> 如何决定顶点是否被裁剪

进行裁剪空间变换后：

正交摄像机裁剪判断方式为$-1\leq x \leq 1, -1\leq y \leq 1, -1\leq z \leq 1$。

透视摄像机裁剪判断方式为$-w\leq x\leq w,-w\leq y\leq w,-w\leq z\leq w$。

### 屏幕空间变换

#### 屏幕空间的意义

屏幕空间是计算机图形学中的概念，指渲染结果在屏幕上显示的坐标空间。

三维坐标经过一系列转换后会转换到最终的二维屏幕坐标空间中，使得图像可以在屏幕上进行展示。

屏幕空间的主要意义是屏幕空间中对应的位置信息是真正的像素位置，而不是虚拟的三维坐标。有了相对屏幕空间的坐标位置，才能准确的控制屏幕上像素点的显示效果。

#### 屏幕空间中的注意事项

在Unity中，屏幕空间左下角为像素坐标(0,0)点，右上角为像素坐标(分辨率宽，分辨率高)。

#### 屏幕空间变换指什么

屏幕空间变换指的是将模型空间中的点或向量从裁剪空间中变换到屏幕空间中，它是顶点变换的第四步，就是将数据从裁剪空间->屏幕空间进行变换。

#### 屏幕空间变换的注意事项

屏幕空间变换将三维坐标中的x和y分量映射到屏幕上，而z分量一般会被用于深度缓冲，之后用于深度测试等。

#### 如何进行屏幕空间变换

首先进行透视除法， 保证裁剪空间中点的数据表达在齐次裁剪空间的范围内。

假设裁剪空间中的点为$(X_{clip},Y_{clip},Z_{clip},W_{clip})$，经过透视除法之后为$(\frac {X_{clip}} {W_{clip}},\frac {Y_{clip}} {W_{clip}},\frac {Z_{clip}} {W_{clip}},\frac {W_{clip}} {W_{clip}})$。

然后找到齐次裁剪空间和屏幕空间的映射关系。

| ![[Pasted image 20240617211815.png]] | ![[Pasted image 20240617211836.png]] |
| ------------------------------------ | ------------------------------------ |

根据这两个图可以得到$X_屏=\frac{Width}2\times X_齐+\frac{Width}2$，$Y_屏=\frac{Height}2\times Y_齐+\frac{Height}2$。

代入齐次坐标后得到$X_屏=\frac{Width\times {X_{clip}}}{2\times W_{clip}}+\frac{Width}2, Y_屏=\frac{Height\times {Y_{clip}}}{2\times W_{clip}}+\frac{Height}2$。

## 语法基础——Shaderlab语法

### 材质和Shader

#### Unity Shader和Shader的区别

Shader是一个更通用的概念，用于描述图形渲染程序中的着色器程序，而Unity Shader是特指在Unity中使用的着色器。

可以认为Unity Shader是对Shader的一种封装，是对底层图形渲染技术的封装，它提供了一种叫做ShaderLab的语言，来让我们更加轻松地编写和管理着色器。

#### Unity中的材质和Shader

如果想要在Unity中体现出一个Shade的渲染效果，必须配合使用材质和Shader才能达到目标。

一般的使用流程是：

1. 创建材质
2. 创建Shader，把该Shader赋给材质
3. 把材质赋给指定的对象
4. 在材质面板中调整Shader的相关属性，以达到目标效果

#### 创建材质

在Project窗口右键Create->Material即可创建材质。

选中材质后，在Inspector窗口中的Shader选项可以选择指定的Shader进行使用，下方的内容是选中Shader提供的可编辑变化的相关变量，它们会直接影响渲染结果。

关联好Shader之后，将材质赋值给GameObject上的Mesh Renderer等相关的渲染器组件上就可以了。

#### 创建Shader

在Project窗口右键Create->Shader即可创建Shader。

1. Standard Surface Shader(标准曲面着色器)：包含标准光照模型的表面着色器模板。
2. Unlit Shader：不包含光照的基本顶点/片元着色器。
3. Image Effect Shader：用于实现屏幕后处理效果的基本模板。
4. Compute Shader：利用GPU并行计算一些和常规渲染流水线无关的内容。
5. Ray Tracing Shader：用于实现光线追踪效果的着色器。

### ShaderLab的基本结构

ShaderLab是用来编写Unity Shader的一种语言，一种Unity自定义的语法规则，它提供一种结构化的方式来描述Unity着色器的各个部分。

无论我们编写那种类型的Shader，或者选择哪些语言去编写Shader，在Unity中总是要通过ShaderLab语言对其进行包装和组织。

ShaderLab主要由4个部分组成：

1. Shader的名字
2. Shader的属性
3. 若干个子着色器
4. 备用的Shader

```C
Shader "Shader名称"
{
    Properties
    {
        // 材质面板上可以看到的属性
    }

    SubShader
    {
		// 顶点-片元着色器
		// 表面着色器
		// 固定函数着色器
    }

	SubShader
    {
		// 更加精简的版本（目的是适配旧设备）
    }

	Fallback "备用Shader"
}
```

#### Shader的名字

1. 直接修改Shader文件中Shader后的名字即可
2. Shader的名字决定了在材质面板的选择路径

注意：不能使用中文命名。

#### Shader的属性

在Shader编写时我们经常会用到不同类型的变量或贴图等资源，为了增加Shader的可调节性，有些变量不会直接在Shader程序中写死，而是作为开放的属性显示在材质面板上，供我们在使用时调节，这些开放的属性就是通过属性来定义的。

Shader的属性具有两个特点：

1. 可以在材质面板中编辑
2. 可以在后续当作输入变量提供给所有子着色器使用

> 基本语法

在Shader文件中，Shader属性是存在于Shader语句块中的Properties属性语句块中的，只需要在Properties语句块中按照语法规则声明属性即可。

Unity Shader的属性主要分为三大类：数值、颜色和向量、纹理贴图。

属性的基本语法：

```C
_Name("DisplayName", type) = defaultValue[{options}]
```

- \_Name：属性的名字，以下划线开头
- DisplayName：材质面板上显示的名字
- type：属性的类型
- defaultValue：将Shader指定给材质时初始化的默认值

> 数值类型

数值类型有三种：**Int**、**Float**、**Range**。

Unity Shader中的数值类型属性基本都是Float类型的，Int类型在编译时都会转换为Float类型，所以使用的更多的是Float类型。

> 颜色和向量类型

颜色和向量都可以用一个四个数组成的类型来表示。

颜色类型是**Color**，由RGBA四个分量组成，每个分量的取值范围是0~1。

向量类型是**Vector**，由XYZW四个分量组成，没有分量的取值没有范围限制。

> 纹理贴图类型属性

纹理贴图类型有四种：

1. **2D**：最常用的纹理，漫反射贴图、法线贴图都属于2D纹理。
2. **2DArray**：纹理数组，允许在纹理中存储多层图像数据，每层看作一个2D图像，一般使用脚本创建，较少使用，了解即可。
3. **Cube**：立方体纹理，由前后左右上下6张有联系的2D贴图拼成的里房里，比如天空盒和反射探针。
4. **3D**：一般使用脚本创建，极少使用，了解即可。

defaultValue默认值有white、black、gray、bump、red，默认值后面的{}为固定写法。

#### Shader的子着色器

每一个Shader都至少包含一个SubShader，当Unity显示一个物体时，就会在Shader文件中检测这些SubShader语句块，然后选择第一个能够在当前显卡上运行的SubShader来执行。

在Shader中实现一些高级效果时，为了避免在某些设备上无法执行，可以写多个SubShader来适配这些低端设备。

SubShader中包含最终的渲染相关代码，决定了最终的渲染效果。

> SubShader的基本构成

SubShader主要由三部分构成：

1. 渲染标签：通过标签来确定什么时候以及如何对物体进行渲染
2. 渲染状态：通过状态来确定渲染时的剔除方式、深度测试方式、混合方式等等内容
3. 渲染通道：具体实现着色器代码的地方（每个SubShader语句块中至少有一个渲染通道，可以有多个）。

##### 渲染标签Tags

渲染标签是通过键值对的形式进行声明的，没有数量限制，其语法结构为：`Tags{"标签名1"="标签值1", ……}`。

1. **Queue**：渲染队列
	1. Background（1000）：最早被渲染的物体的队列，一般用来渲染天空盒和背景
	2. Geometry（2000）：不透明的几何体通常使用该队列，当没有声明渲染队列时，Unity会默认使用这个队列。
	3. AlphaTest（2450）：有透明通道的，需要进行Alpha测试的几何体会使用该队列，当所有GEometry队列实体绘制完后再绘制AplphaTest队列，效率更高。
	4. Transparent（3000）：该队列中几何体按照由远到近的顺序进行绘制，半透明物体的渲染队列，所有进行透明混合的几何体都应该使用该队列，比如玻璃材质、粒子特效等。
	5. Overlay（4000）：放在最后渲染的最烈，用于叠加渲染的效果，比如镜头光晕等。
	6. 自定义队列：基于Unity预先定义的渲染队列标签进行加减运算来定义自己的渲染队列，比如Geometry+1，Transparent-1。
2. **RenderType**：渲染类型
	1. Opaque：用于普通Shader，比如不透明、自发光、反射等。
	2. Transparent：用于半透明Shader，比如透明、粒子。
	3. TransparentCutout：用于透明测试Shader，比如植物叶子。
	4. Background：用于天空盒Shader。
	5. Overlay：用于GUI纹理、Halo（光环）、Flare（光晕）。
3. **DisableBatching**：禁用批处理。当使用批处理时，模型会被变换到世界空间，模型空间会被丢弃，这可能会导致某些使用模型空间顶点数据的Shader最终无法实现想要的效果，可以通过开启禁用批处理来解决该问题。
4. **DisableFading**：静止投射阴影。用于控制物体是否投射阴影。
5. **IgnoreProjector**：忽略投影机。用于控制物体是否受到投影机的投射，一般半透明Shader需要启用该标签。
6. **CanUseSpriteAtlas**：是否用于精灵。
7. **PreviewType**：预览类型。材质在预览窗口默认为球形，如果想要改变为平面或天空盒，可以设置该标签。
	1. Pannel：平面。
	2. ShyBox：天空盒。

注意事项：以上标签只能在SubShader语句块中声明，Pass语句块中有专门的标签类型。

##### 渲染状态States

渲染状态的语法结构为：`渲染状态 状态类型`，如果存在多个渲染状态，可以通过空行隔开。

> Cull（剔除方式）

用于设置多边形的剔除方式，有背面剔除、正面剔除、不剔除。

- Back：背面剔除，背面不渲染。
- Front：正面剔除，正面不渲染。
- Off：不剔除，都渲染。

> ZWrite（写入深度缓冲）

用于设置是否写入深度缓冲，默认为写入。

- On：写入。
- Off：不写入。

深度缓冲是一个与屏幕像素对应的缓冲区，用于存储每个像素的深度值（距离相机的距离）。

在渲染场景之前，深度缓冲被初始化为最大深度值，表示所有像素都在相机之外，最后留在深度缓冲中的信息会被渲染。

一般情况下，在做透明等特殊效果时，会设置为不写入。

> ZTest（深度测试）

用于设置深度测试的对比方式，默认为LEqual。

- Less：小于当前深度缓冲区中的值，则通过测试。
- Greater：大于当前深度缓冲区中的值，则通过测试。
- LEqual：小于等于当前深度缓冲区中的值，则通过测试。
- GEqual：大于等于当前深度缓冲区中的值，则通过测试。
- Equal：等于当前深度缓冲区中的值，则通过测试。
- NotEqual：不等于当前深度缓冲区中的值，则通过测试。
- Always：始终通过测试。

深度测试的主要目的是确保在渲染时，像素按照正确的深度（距离相机的距离）顺序进行绘制，从而创建正确的遮挡关系和透视效果。

深度测试流程：如果开启了深度测试，则比较片元的深度值和已经存在于深度缓冲区中的深度值，得到深度测试结果，若通过了深度测试，则该片元就会被渲染出来，并可以选择是否写入深度缓冲区，若没有通过测试，则该片元会被丢弃。

在渲染场景之前，深度缓冲被初始化为最大深度值，表示所有像素都在相机之外。

在渲染过程中，对于每个像素，深度测试会将当前像素的深度值与深度缓冲区中对应位置的值进行比较。

一般情况下，只有在实现一些特殊效果时才会修改深度测试方式，比如透明物体渲染会修改为Less，描边效果会修改为Greater。

> Blend（混合方式）

用于设置渲染图像的混合方式（多种颜色叠加混合，比如透明、半透明效果和遮挡的物体进行混合），默认不混合。

- One One：线性减淡。
- SrcAlpha OneMinusSrcAlpha：正常透明混合。
- OneMinusDstColor One：滤色。
- DstColor Zero：正片叠底。
- DstColor SrcColor：X光片效果。
- One OneMinusSrcAlpha：透明度混合。

混合流程：如果开启了混合，则会将片元的颜色值和颜色缓冲区中的颜色值进行混合，再更新到颜色缓冲区中，否则直接使用片元的颜色值。

> LOD

用于设置LOD级别，在不同距离下使用不同的渲染方式处理。

> ColorMask

用于设置颜色通道的写入蒙版，默认蒙版为RGBA。

---

注意事项：以上这些状态不仅可以在SubShader语句块中使用，还可以在Pass渲染通道语句块中使用。SubShader中的渲染状态会影响之后的所有Pass，而Pass中的渲染状态不会影响其他Pass。

##### 渲染通道Pass

渲染通道的语法结构为：

```C
Pass
{
	1. 名称
	2. 渲染标签
	3. 渲染状态
	4. 其他着色器代码
}
```

> Pass的名字

对Pass命名的主要目的是，可以利用UsePass命令在其他Shader当中复用该Pass的代码，只需要在其他Shader当中使用`UsePass "Shader路径/Pass名"`。

注意：Unity内部会把Pass名称转换为大写字母，因此在使用UsePass命令时必须使用大写形式的名字。

> Pass中的渲染标签

Pass中的渲染标签语法和SubShader中相同，但是SubShader中的渲染标签不能在Pass中使用。

1. **LightMode**：指定该Pass在哪个阶段执行。可以将着色器代码分配给适当的渲染阶段，以实现所需的效果。
2. **RequireOptions**：指定当满足某些条件时才渲染该Pass。目前只支持SoftVegetation，仅当Quality窗口中开启了SoftVegetation时才渲染此通道。
3. **PassFlags**：一个渲染通道Pass可指示一些标志来更改渲染管线向Pass传递数据的方式。目前只支持OnlyDirectional，在ForwardBase向前渲染的通道类型中使用时，此标志的作用是仅允许主方向光和环境光/光照探针数据传递到着色器，这意味着非重要光源的数据将不会传递到顶点光源或球谐函数着色器变量。

> Pass中的渲染状态

在SubShader中的渲染状态都可以在Pass中使用，在SubShader语句块中使用的渲染状态会影响后续的所有渲染通道。

Pass中还可以使用固定管线着色器的命令。

> 其他着色器代码

该部分就是实现着色器的核心代码部分，可能会用到CG或HLSL等着色器语言来编写。

> GrabPass命令

利用该命令可以将即将绘制对象时的屏幕内容抓取到纹理中，在后续通道中即可使用此纹理，从而执行基于图像的高级效果。

举例：

```C
GrabPass
{
	"_BackgroundTexture"
}
```

该命令一般写在Pass之前，在之后的Pass代码中就可以利用_BackgroundTexture变量进行处理。

#### Shader的备用着色器

在ShaderLab中允许有多个SubShader，当执行渲染时，会从上到下使用第一个能够正常执行的子着色器来渲染对象。

备用着色器的主要作用就是当所有Shader文件中的所有子着色器都无法正常执行时，让物体能够使用一个最低级的Shader去渲染，虽然效果较差，但至少保证它能够显示。

备用着色器的语法为：`Fallback "Shader名"`，若声明`Fallback Off`则表示关闭备用着色器。

### Shader的形式

#### 表面着色器

表面着色器是Unity自己创造的一种着色器代码类型，它的本质是对顶点/片元着色器的一个封装。其特点如下：

1. 直接在SubShader语句块中书写着色器逻辑。
2. 不需要关心也不需要使用多个Pass，Unity会帮助我们处理每个Pass是如何渲染的。
3. 可以使用CG或HLSL两种语言编写Shader逻辑。
4. 代码量较少，可控性较低，性能消耗较高。
5. 适用于处理需要和各种光源打交道的着色器（主机、PC平台）。

#### 顶点/片元着色器

顶点/片元着色器的特点是：

1. 需要在Pass渲染通道中编写着色器逻辑。
2. 可以使用CG或HLSL两种Shader语言编写Shader逻辑。
3. 代码量较多，灵活性较强，性能消耗更可控，可以实现更多渲染细节。
4. 适用于光照处理较少，自定义渲染效果较多时（移动平台首选）。

#### 固定函数着色器

一些旧设备不支持可编程管线着色器，就需要使用固定函数着色器来进行渲染，其特点是：

1. 需要在Pass渲染通道中编写着色器逻辑。
2. 需要使用ShaderLab语法中的渲染设置命令来编写，而非CG和HLSL语言。

由于不支持可编程管线着色器的旧设备基本已经不存在了，所以固定函数着色器也几乎不会再使用了。

即使在Unity中编写固定函数着色器，在内部也会被编译为顶点/片元着色器，因此真正意义上的固定函数着色器已经不存在了。

## 语法基础——CG语法

对于顶点/片元着色器来说，CG语句需要写在Pass渲染通道语句块内，我们需要在Pass语句块中加入指令：

```C
CGPROGRAM

ENDCG
```

然后将CG代码写在两个指令之间的位置。

> 重要的编译指令——指定着色器函数

在真正编写CG代码之前，首先需要使用`#pragma`声明编译指令，定义实现顶点/片元着色器代码的函数名称。

```C
#pragma vertex name
#pragma fragment name
```

### 数据类型
#### 基础数据类型

- uint：32位无符号整型。
- int：32位整型。
- float：32位浮点数。
- half：16位浮点数。
- fixed：12位浮点数。
- bool：布尔类型。
- string：字符串。
- sampler：通用的纹理采样器，可以用于处理各种不同维度和类型的纹理。
	- 1D：一维纹理，通常用于对一维纹理进行采样，例如从左到右的渐变色。
	- 2D：二维纹理，用于处理二维图像纹理，例如贴图。
	- 3D：三维纹理，通常用于体积纹理，例如体积渲染。
	- CUBE：立方体纹理，通常用于处理环境映射等需要立方体贴图的情况。
	- RECT：矩形纹理，通常用于一些非标准的纹理映射需求。

#### 基础复合数据类型

> 数组

- 一维：`int a[4] = {1, 2, 3, 4}`
- 二维：`int b[2][3] = {{1, 2, 3}, {4, 5, 6}}`

不能直接通过变量获取数组的长度，只能在声明时记录数组的长度。

> 结构体

- 和C#基本一样，但没有访问修饰符。
- 结构体声明结束加分号。
- 一般在函数外声明。

```C
sturct Test
{
	int a;
	bool b;
	string c;
};
```

#### 特殊数据类型

> 向量

向量类型是基于基础数据类型声明的，向量的最大维度不超过4维，数据类型可以是任意的数值类型，如`fixed2 f2 = fixed2(1.2, 2.5)`。

> 矩阵

矩阵的最大行列不大于4，且不小于1，数据类型可以是任意的数值类型，如`int2x3 m = {1,2,3,4,5,6}`。

> 布尔类型的特殊使用

bool类型同样可以用于如同向量一样声明，可以用于存储一些逻辑判断结果，例如：

```C
float3 a = float3(0.5, 0.0, 1.0);
float3 b = float3(0.6, -0.1, 0.9);
bool3 c = a < b; // c的结果是(true, false, false)
```

### Swizzle操作符

Swizzle操作符用于获取向量中的元素，通常以点号的形式使用，后面跟着所需的分量顺序。

对于四维向量来说，可以通过`向量.xyzw`或`向量.rgba`两种写法来表示向量中的四个值，xyzw和rgba分别代表四维向量中的四个元素。

#### 使用方法

```C
fixed4 f4 = fixed4(1,2,3,4);
```

> 提取分量

`fixed f = f4.w`

> 重新排列分量

`f4 = f4.yzxw`，`f4 = f4.abgr`

> 创建新的向量

```C
fixed3 f3 = f4.xyz;
fixed2 f2 = f4.xz;
fixed4 f4_2 = fixed4(f2, 4, 5);
```

#### 向量和矩阵的更多用法

> 利用向量声明矩阵

```C
fixed4x4 f4x4 =
{
	fixed4(1, 2, 3, 4),
	fixed4(5, 6, 7, 8),
	fixed4(9, 10, 11, 12),
	fixed4(13, 14,15, 16)
};
```

> 获取矩阵中的元素

```C
fixed f = f4x4[0][0];
```

> 利用向量获取矩阵中的某一行

```C
fixed2 f2 = f4x4[0];
```

> 高维转低维

```C
fixed3 f3_2 = f4;
fixed3x3 f3x4 = f4x4;
```

### 运算符

#### 比较运算符

<, >, <=, >=, == , !=

#### 条件运算符

```C
condition ? value_if_true : value_if_false
```

#### 逻辑运算符

||, &&, !

#### 数学运算符

+, -, \*, /, %, ++, --

### 流程控制语句

CG语法中的流程控制语句和C#中的一模一样，需要注意的是：

1. 尽量少地使用循环语句，如果一定要用，则要减少次数和复杂度。
2. 要利用GPU并行性这一特点来代替循环。
3. 尽量避免复杂的条件分支。

#### 条件分支语句

1. if语句
2. switch语句

#### 循环语句

1. for
2. while
3. do while

### 函数

#### 无返回值函数

```C
void name(in 类型 名称, out 类型 名称)
{
	// 函数体
}
```

- in：表示是输入参数，由函数外部传递给内部，内部不会修改该参数，只会使用该参数进行计算，允许有多个。
- out：表示是输出参数，由函数内部传递给调用者，在函数内部必须对该参数值进行初始化或修改，允许有多个。

in和out可以省略，但是为了提高可读性和可维护性，建议不要省略。

#### 有返回值函数

```C
type name(in 类型 名称)
{
	// 函数体
	return 返回值;
}
```

虽然可以在有返回值的函数中使用out参数，但是这并不是常见做法，除非是一些自定义逻辑函数，对于顶点/片元着色器函数只会使用单返回值的方式进行处理。

### 顶点/片元着色器的基本结构

#### 顶点着色器

```C
// POSITION和SV_POSITION是CG语言的语义
// POSIITON：把模型的顶点坐标填充到输入的参数v当中
// SV_POSITION：顶点着色器输出的内容是裁剪空间中的顶点坐标
// 如果没有这些语义来限定输入和输出参数的话，那么渲染器就完全不知道输入和输出是什么，然后得到错误的结果
float4 vert(float4 v: POSITION): SV_POSITION
{
	// mul是CG提供的矩阵和向量的乘法运算函数
	// mul(UNTIY_MATRIX_MVP, v);
	// UnityObjectToClipPos的作用和上面的矩阵乘法是一样的，主要目的就是进行坐标变换
	return UnityObjectToClipPos(v);
}
```

#### 片元着色器

```C
// SV_TARGET：告诉渲染器，把输出颜色存储到一个渲染目标中，这里将输出到默认的帧缓存中
fixed4 frag(): SV_TARGET
{
   return fixed4(1,0,0,1); 
}
```

### 语义

CG语言中提供了语义这种特殊关键字，用于修饰函数中的传入参数和返回值。

语义的主要作用就是让Shader知道从哪里读取数据，以及把数据输出到哪里。


> 应用阶段->顶点着色器

该部分是应用阶段传递模型数据给顶点着色器时，Unity支持的语义。一般在顶点着色器回调函数的传入参数中应用。

- **POSITION**：模型空间中的顶点位置，通常是float4类型。
- **NORMAL**：顶点法线，通常是float3类型。
- **TANGENT**：顶点切线，通常是float4类型。
- **TEXCOORDn**：顶点的纹理坐标，通常是float2或float4类型。纹理坐标也称为UV坐标，表示该顶点对应纹理图像上的位置。
- **COLOR**：顶点颜色，通常是fixed4或float4类型。

> 顶点着色器->片元着色器

该部分是从顶点着色器传递数据给片元着色器时，Unity支持的语义。一般在顶点着色器回调函数的返回值中应用。

- SV_POSITION：裁剪空间中的顶点坐标（必备）。
- COLOR0：通常用于输出第一组顶点颜色（非必须）。
- COLOR1：通常用于输出第二组顶点颜色（非必须）。
- TEXCOORD0~TEXCOORD7：通常用于输出纹理坐标（非必须）。

> 片元着色器输出

该部分是片元着色器输出时，Unity支持的语义。一般在片元着色器回调函数的返回值中应用。

- SV_TARGET：输出值会存储到渲染目标中。

### 顶点/片元着色器传递更多参数

#### 顶点着色器获取更多数据信息

当我们在顶点着色器中想要获取更多模型相关信息时，可以使用结构体对数据进行封装，通过对结构体中成员变量加语义的方式来定义想要获取的信息。

```C
struct a2v 
{
	// 顶点坐标（基于模型空间）
	float4 vertex: POSITION;
	// 顶点法线（基于模型空间）
	float3 normal: NORMAL;
	// 纹理坐标
	float2 uv: TEXCOORD0;
}

float4 vert(a2v data): SV_POSITION
{
	return UnityObjectToClipPos(data.vertex);
}
```

#### 片元着色器获取更多数据信息

当我们在片元着色器中想要获取更多信息时，还是采用封装结构体的方式。

注意：片元着色器中获取的数据基本上都是由顶点着色器传递过来的，所以我们封装的结构体还需要作为顶点着色器的返回值类型。

```C
struct a2v 
{
	// 顶点坐标（基于模型空间）
	float4 vertex: POSITION;
	// 顶点法线（基于模型空间）
	float3 normal: NORMAL;
	// 纹理坐标
	float2 uv: TEXCOORD0;
}

struct v2f
{
	// 裁剪空间下的坐标
	float4 position: SV_POSITION;
	// 顶点法线（基于模型空间）
	float3 normal: NORMAL;
	// 纹理坐标
	float2 uv: TEXCOORD0;
}

v2f vert(a2v data)
{
	// 需要传递给片元着色器的数据
	v2f v2fData;
	v2fData.position = UnityObjectToClipPos(data.vertex);
	v2fData.normal = data.normal;
	v2fData.uv = data.uv;

	return v2fData;
}

// SV_TARGET：告诉渲染器，把输出颜色存储到一个渲染目标中，这里将输出到默认的帧缓存中
fixed4 frag(v2f data): SV_TARGET
{
   return fixed4(1,0,0,1); 
} 
```

### ShaderLab属性类型和CG变量类型的匹配关系

| ShaderLab属性类型 | CG变量类型                |
| ------------- | --------------------- |
| Color, Vector | float4, half4, fixed4 |
| Range, Float  | float, half, fixed    |
| 2D            | sampler2D             |
| Cube          | samplerCube           |
| 3D            | sampler3D             |
| 2DArray       | sampler2DArray        |

> 如何在CG语句块中使用ShaderLab中声明的属性

直接在CG语句块中声明和属性中对应类型的同名变量即可。

### CG内置函数

Unity Shader中的CG语言提供了各种用于图形编程的函数，这些函数是CG为我们封装好的逻辑，我们可以使用它们来编写Unity Shader。

#### 数学函数

> 三角函数相关

- sincos(floatx, out s, out c)：同时计算x值的sin值和cos值进行返回，比分别运算快得多。
- sin(x)：正弦函数
- cos(x)：余弦函数
- tan(x)：正切函数
- sinh(x)：双曲正弦函数
- cosh(x)：双曲余弦函数
- tanh(x)：双曲正切函数
- asin(x)：反正弦函数
- acos(x)：反余弦函数
- atan(x)：反正切函数
- atan2(y, x)：计算y/x的反正切值。

> 向量、矩阵相关

- cross(A, B)：叉乘
- dot(A, B)：点乘
- mul(M, N)：计算两个矩阵相乘
- mul(M, v)：计算矩阵和向量相乘
- mul(v, M)：计算向量和矩阵相乘
- transpose(M)：计算矩阵的转置
- determinant(m)：计算矩阵的行列式因子

> 数值相关

- abs(x)：绝对值
- ceil(x)：向上取整
- ……

#### 几何函数

- length(v)：返回向量的模长
- normalize(v)：归一化向量
- distance(p1, p2)：计算两点之间的距离
- refelct(I, N)：计算反射光方向向量，I为入射光，N为顶点法向量。I是指向顶点的，I和N必须归一化，必须是三维向量。
- refract(I, N, eta)：计算折射向量，I为入射光，N为顶点法向量，eta为折射系数。I是指向顶点的，I和N必须归一化，必须是三维向量。

#### 纹理函数

纹理采样函数返回值为fixed4类型的颜色值。

- tex2D(sampler2D tex, float2 s)：二维纹理查询。
- tex2D(sampler2D tex, float2 s, float2 dsdx, float2 dsdy)：使用导数值查询二维纹理。
- tex2D(sampler2D tex, float2 sz)：二维纹理查询，并进行深度值比较。
- tex2D(sampler2D tex, float2 sz, float2 dsdx, float2 dsdy)：使用导数值查询二维纹理，并进行深度值比较。
- tex2Dproj(sampler2D tex, float3 sq)：二维投影纹理查询。
- tex2Dproj(sampler2D tex, float3 sq)：二维投影纹理查询。

### CG内置文件

可以在Unity的安装目录Editor->Data->CGIncludes中找到CG内置文件，后缀为cginc的文件是CG语言内置文件，后缀为glslinc的文件为GLSL语言内置文件。

内置文件是预定义的Shader文件，其中包含了一些已经写好的Shader逻辑，作用和CG内置函数一样，可以提升开发效率。

Unity中常用的内置文件有：

1. UnityCG.cginc
2. Lighting.cginc
3. UnityShaderVariables.cginc
4. HLSLSupport.cginc

> 如何使用CG内置文件

在CG语句块中通过编译指令`#include "内置文件名.cginc"`的形式进行引用。

注意：一些常用的函数、宏、变量可以不引用，Unity会在编译时自动识别，但是为了避免报错，建议都引用。

#### 常用内容

> 方法

1. `float3 WorldSpaceViewDir(float4 v)`：输入一个模型空间中的顶点位置，返回世界空间中从该点到摄像机的观察方向。
2. `float3 ObjSpaceViewDir(float4 v)`：输入一个模型空间中的顶点位置，返回模型空间中从该点到摄像机的观察方向。
3. `float3 WorldSpaceLightDir(float4 v)`：仅用于前向渲染中，输入一个模型空间中的顶点位置，返回世界空间中从该点到光源的光照方向。
4. `float3 ObjSpaceLightDir(float4 v)`：仅用于前向渲染中，输入一个模型空间中的顶点位置，返回模型空间中从该点到光源的光照方向。
5. `float3 UnityObjectToWorldNormal(float3 norm)`：把法线方向从模型空间转换到世界空间中。
6. `float3 UnityObjectToWorldDir(in float3 dir)`：把方向矢量从模型空间转换到世界空间中。
7. `float3 UnityWorldToObjectDir(float3 dir)`：把方向矢量从世界空间转换到模型空间中。

> 结构体

1. `appdata_base`：用于顶点着色器输入，顶点位置、顶点法线、第一组纹理坐标。
2. `appdata_tan`：用于顶点着色器输入，顶点位置、顶点法线、顶点切线、第一组纹理坐标。
3. `appdata_full`：用于顶点着色器输入，顶点位置、顶点法线、顶点切线、四组（或等多）纹理坐标。
4. `appdata_img`：用于顶点着色器输入，顶点位置、第一组纹理坐标。
5. `v2f_img`：用于顶点着色器输出，裁剪空间中的位置、纹理坐标。

> 矩阵变换

模型空间->世界空间->观察空间->裁剪空间->屏幕空间。

1. `UNITY_MATRIX_MVP`：用于从模型空间变换到裁剪空间。
2. `UNITY_MATRIX_MV`：用于从模型空间变换到观察空间。
3. `UNITY_MATRIX_V`：用于从世界空间变换到观察空间。
4. `UNITY_MATRIX_P`：用于从观察空间变换到裁剪空间。
5. `UNITY_MATRIX_VP`：用于从世界空间变换到裁剪空间。
6. `UNITY_MATRIX_T_MV`：UNITY_MATRIX_MV的转置矩阵。
7. `UNITY_MATRIX_IT_MV`：UNITY_MATRIX_MV的逆转置矩阵，用于将法线从模型空间变换到观察空间。
8. `Object2World`：当前的模型矩阵，用于从模型空间变换到世界空间。
9. `World2Object`：Object2World的逆矩阵，用于从世界空间变换到模型空间。

> 变量

1. `_Time`：自关卡加载以来的时间，用于对着色器内的事物进行动画处理。不用引用，直接使用即可。
2. `_LightColor0`：光的颜色。

# Shader入门知识

## 光照模型

光照模型指的是用于模拟光照效果的一组数学公式和算法，用于确定在3D场景中的模型表面应该如何对光进行反射和散射，从而实现视觉上逼真的照明效果。

### 漫反射光照模型

#### 逐顶点光照和逐片元光照

> 逐顶点光照

逐顶点光照在顶点着色器中进行计算，在物体的每个顶点上进行光照计算，顶点之间的内部区域使用插值来获得颜色信息。

优点：计算量较小，通常在移动设备上性能较好，适用于移动游戏等性能要求高的场景。

缺点：照明效果不够精细，特别是在物体表面上的细节区域，因为颜色插值可能不足以捕捉到细微的照明变化。

使用场景：逐顶点光照适用于需要在有限资源下获得较好性能的场景，例如移动游戏。

> 逐片元光照

逐片元光照在片元着色器中进行计算，在每个像素上进行光照计算。

优点：精细度更高，可以捕捉到物体表面上的细微照明变化，提供更逼真的效果。

缺点：计算量大，对于像素密集的场景需要更多的计算资源。

使用场景：逐片元光照通常用于需要高质量照明效果的主机游戏和PC，以及要求视觉逼真度较高的场景。

> 关于逐顶点光照的插值运算

插值运算是由GPU进行处理的，这个过程被高度优化过，所以在实时渲染中是非常高效的。

假设A，B，C分别为三角面片的顶点，P为该三角面片中的任一像素。

首先会计算出P相对于三个顶点的位置权重，然后使用这个权重来计算P的颜色。

$$
PixelColor_P = Weight_A \times Color_A + Weight_B \times Color_B + Weight_C \times Color_C
$$

#### 兰伯特光照模型

> 颜色相乘

颜色相乘通常用于计算光照、材质混合、纹理混合等需求，是一种用于混合颜色的操作，计算结果可以理解为两种颜色叠加在一起的效果。

在颜色相乘中，通常是使用颜色通道的值来执行逐通道的乘法。

> 漫反射概念

漫反射是光线撞击一个物体表面后以各个方向均匀地反射出去的过程，在漫反射下，光线以无规律的方式散射，而不像镜面反射那样按照特定的角度反射，这种散射导致了物体表面看起来均匀而不闪烁的效果。

> 兰伯特光照模型的来历和原理

兰伯特光照模型也称为朗伯反射模型，是由瑞士数学家约翰·海因里希·朗伯于1760年左右首次提出。

兰伯特光照模型描述了漫反射表面对光线的反射行为，它称为计算机图形学和渲染中重要的基础模型之一。

原理：漫反射光的强度仅与入射光的方向和反射点处表面法线的夹角的余弦成正比。

> 兰伯特光照模型公式

$$
漫反射光照颜色=光源颜色\times 材质颜色\times max(0, 标准化的物体表面法线向量\cdot 标准化的光源方向向量)
$$

- 标准化的物体表面法线向量点乘标准化的光源方向向量的结果就是$\cos \theta$。
- $max(0,\cos\theta)$的目的是避免负数，对于模型背面，认为照射不到光，直接乘以0变为黑色。

>在Shader中获取公式中的关键信息

1. 光源颜色：Lighting.cginc中的_LightColor0
2. 光源方向：\_WorldSpaceLightPos0，表示光源0在世界坐标系下的位置
3. 向量归一化方法：normalize
4. 取最大值方法：max
5. 点乘方法：dot
6. 兰伯特光照模型环境光变量（用于模拟环境光对物体的影响，避免物体阴影部分全黑）：UNITY_LIGHTMODEL_AMBIENT.rgb
7. 将法线从模型空间转换到世界空间：UnityObjectToWorldNormal

> 逐顶点光照

```C
Shader "Custom/Lambert"  
{  
    Properties  
    {  
        _MainColor("MainColor", Color) = (1,1,1,1)  
    }    SubShader  
    {  
        Pass  
        {  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
  
            struct v2f   
{  
                float4 pos: SV_POSITION;  
                fixed3 color: COLOR;  
            };  
            fixed4 _MainColor;  
  
            v2f vert (appdata_base v)  
            {                v2f o;  
                // 将模型空间的顶点坐标转换到裁剪空间  
                o.pos = UnityObjectToClipPos(v.vertex);  
                // 将模型空间的法线转换到世界空间  
                float3 normal = UnityObjectToWorldNormal(v.normal);  
                // 光照方向  
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);  
                // 使用Lambert模型计算  
                fixed3 color = _LightColor0.rgb * _MainColor.rgb * max(0, dot(normal, lightDir));  
                // 加上环境光，避免阴影处全黑  
                o.color = color + UNITY_LIGHTMODEL_AMBIENT.rgb;  
  
                return o;  
            }  
            fixed4 frag (v2f i) : SV_Target  
            {  
                return fixed4(i.color, 1);  
            }            ENDCG  
        }  
    }}
```

> 逐片元光照

```C
Shader "Custom/LambertF"
{
    Properties
    {
        _MainColor("MainColor", Color) = (1,1,1,1)
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos: POSITION;
                float3 normal: NORMAL;
            };

            fixed4 _MainColor;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.normal = UnityObjectToWorldNormal(v.normal);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);
                fixed3 color = _LightColor0.rgb * _MainColor.rgb * max(0, dot(lightDir, i.normal));
                color += UNITY_LIGHTMODEL_AMBIENT.rgb;
                return fixed4(color, 1);
            }
            ENDCG
        }
    }
}
```

#### 半兰伯特光照模型

>  半兰伯特光照模型的来历

半兰伯特光照模型是基于兰伯特光照模型进行改进的，没有物理依据，只是一个视觉加强的技术。

半兰伯特光照模型出现的主要原因是：在使用兰伯特光照模型时，背光面是全黑的，而半兰伯特光照模型可以让背光面也有明暗变化。

> 半兰伯特光照模型的公式

$$
漫反射光照颜色=光源颜色\times 材质颜色\times ((标准化的物体表面法线向量\cdot 标准化的光源方向向量) * 0.5 + 0.5)
$$

### 高光反射光照模型

#### Phong式高光反射光照模型

> 来历

高光反射光照模型没有特定的发明者，因为有多种计算的方式，和半兰伯特光照模型一样，是经过很多从业者的研究和发展而演化而来的。

比较关键的贡献者为：

- Phong光照模型的提出者：裴祥风（Bui-Tuong Phong），越南裔美国计算机科学家
- Blinn-Phong光照模型的提出者：吉姆·布林（Jim Blinn），美国计算机科学家。

> 原理

Phong式高光反射光照模型的理论是：基于光的反射行为和观察者的位置决定高光反射的表现效果。

认为高光反射的颜色和光源的反射光线以及观察者位置方向向量夹角的余弦成正比，并且通过对余弦值取n次幂来表示光泽度（或反光度）。

>  公式

$$
高光反射光照颜色=光源颜色\times 材质高光反射颜色\times max(0, 标准化后观察方向向量 \dot 标准化后反射方向)^n
$$

> 如何在Shader中获取公式中的关键信息

1. 观察者（摄像机）位置：`WorldSpaceCameraPos
2. 相对于法向量的反射向量方法：`reflect(入射向量，顶点法向量)`
3. 指数幂方法：`pow(底数，指数)`

> 逐顶点

```C
Shader "Custom/Specular"
{
    Properties
    {
        // 高光发射颜色
        _SpecularColor("SpecularColor", Color) = (1,1,1,1)
        // 光泽度
        _Glossiness("Glossiness", Range(0,20)) = 0.5
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 vertex : SV_POSITION;
                fixed3 color: COLOR;
            };

            fixed4 _SpecularColor;
            fixed _Glossiness;

            v2f vert (appdata_base v)
            {
                v2f o;
                // 顶点坐标转换到裁剪空间中
                o.vertex = UnityObjectToClipPos(v.vertex);

                // 获得世界空间下的顶点坐标
                float3 worldVertex = mul(unity_ObjectToWorld, v.vertex);
                // 获得单位化的观察方向向量
                float3 viewDir = normalize(_WorldSpaceCameraPos.xyz - worldVertex);

                // 获得光照方向向量
                float3 lightDir = -normalize(_WorldSpaceLightPos0.xyz);
                // 获得世界空间下的法线
                float3 normal = UnityObjectToWorldNormal(v.normal);
                // 计算反射光线向量
                // float3 reflectionDir = lightDir - 2 * normal * dot(lightDir, normal);
                float3 reflectionDir = reflect(lightDir, normal);
                
                // 使用Phong模型公式计算高光颜色
                fixed3 color = _LightColor0 * _SpecularColor * pow(max(0, dot(viewDir, reflectionDir)), _Glossiness);
                o.color = color;

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                return fixed4(i.color, 1);
            }
            ENDCG
        }
    }
}
```

> 逐片元

```C
Shader "Custom/SpecularF"
{
    Properties
    {
        // 高光发射颜色
        _SpecularColor("SpecularColor", Color) = (1,1,1,1)
        // 光泽度
        _Glossiness("Glossiness", Range(0,20)) = 0.5
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 vertex : SV_POSITION;
                float4 worldVertex: TEXCOORD;
                float3 normal: NORMAL;
            };

            fixed4 _SpecularColor;
            fixed _Glossiness;

            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);
                o.normal = UnityObjectToWorldNormal(v.normal);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                float3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldVertex);
                float3 lightDir = -normalize(_WorldSpaceLightPos0.xyz);
                float3 reflectionDir = reflect(lightDir, i.normal);
                fixed3 color = _LightColor0 * _SpecularColor * pow(max(0, dot(viewDir, reflectionDir)), _Glossiness);
                return fixed4(color, 1);
            }
            ENDCG
        }
    }
}
```

#### Phong光照模型

> 颜色相加

颜色相加和相乘同样是将颜色叠加，但它们是有区别的：

- 相乘：最终颜色往黑色靠拢，计算两个颜色混合时一般用颜色相乘。
- 相加：最终颜色往白色靠拢，计算光照反射时一般用颜色相加。

> Unity Shader中的环境光

之前在计算兰伯特光照后加上了一个环境光变量`UNITY_LIGHTMODEL_AMBIENT`，这个变量是可以在Unity中设置的。

Window->Rendering->Lighting，Environment页签中的Environment Lighting，可以设置环境光的来源。

- Skybox/Color：`UNITY_LIGHTMODEL_AMBIENT`
- Gradient：`unity_AmbientSky`，`unity_AmbientEquator`，`unity_AmbientGround`

> 来历和原理

Phong光照模型是裴祥风提出的一种局部光照经验模型，其认为物体表面反射光线由三部分组成：环境光、漫反射光、镜面反射光。

> 公式

$$
物体表面光照颜色=环境光颜色+漫反射光颜色+高光反射光颜色
$$

- 环境光颜色：`UNITY_LIGHTMODEL_AMBIENT`或`unity_Ambient...`
- 漫反射光颜色：使用兰伯特光照模型计算得到
- 高光反射光颜色：使用Phong式高光反射光照模型计算得到

> 逐顶点

```C
Shader "Custom/Phong"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _SpecularColor ("Specular Color", Color) = (1, 1, 1, 1)
        _Shininess ("Shininess", Range(0.01, 100)) = 1
        _HalfLambert ("Half Lambert", Range(0, 1)) = 0
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 vertex : SV_POSITION;
                fixed4 color: COLOR;
            };
            
            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _Shininess;
            float _HalfLambert;

            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);

                // 计算相关变量
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);
                float3 normal = normalize(UnityObjectToWorldNormal(v.normal));
                float3 reflectionDir = normalize(reflect(-lightDir, normal));
                float3 viewDir = normalize(WorldSpaceViewDir(v.vertex));
                
                // 计算漫反射光，使用兰伯特光照模型
                float4 ldotn = dot(lightDir, normal);
                fixed4 diffuse = _LightColor0 * _MainColor * (_HalfLambert == 0 ? saturate(ldotn) : (ldotn * 0.5 + 0.5));

                // 计算高光反射光，使用Phong式高光反射模型
                float4 rdotv = saturate(dot(reflectionDir, viewDir));
                fixed4 specular = _LightColor0 * _SpecularColor * pow(rdotv, _Shininess);

                // 环境光
                fixed4 ambient = unity_AmbientSky;

                // 最终颜色
                o.color = ambient + diffuse + specular;

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                return i.color;
            }
            ENDCG
        }
    }
}
```

> 逐片元

```C
Shader "Custom/PhongF"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1, 1, 1, 1)
        _SpecularColor ("Specular Color", Color) = (1, 1, 1, 1)
        _Shininess ("Shininess", Range(0.01, 100)) = 1
        _HalfLambert ("Half Lambert", Range(0, 1)) = 0
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float4 vertex : TEXCOORD;
                float3 normal : NORMAL;
            };
            
            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _Shininess;
            float _HalfLambert;

            v2f vert (appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.vertex = v.vertex;
                o.normal = normalize(UnityObjectToWorldNormal(v.normal));
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {               
                // 计算相关变量
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);
                float3 viewDir = normalize(WorldSpaceViewDir(i.vertex));
                float3 reflectionDir = normalize(reflect(-lightDir, i.normal));

                // 计算镜面反射光，使用兰伯特光照模型
                float4 ldotn = dot(lightDir, i.normal);
                fixed4 diffuse = _LightColor0 * _MainColor * (_HalfLambert == 0 ? saturate(ldotn) : (ldotn * 0.5 + 0.5));

                // 计算高光反射光，使用Phong式高光反射模型
                float4 rdotv = saturate(dot(reflectionDir, viewDir));
                fixed4 specular = _LightColor0 * _SpecularColor * pow(rdotv, _Shininess);

                // 环境光
                fixed4 ambient = unity_AmbientSky;

                // 最终颜色
                return ambient + diffuse + specular;
            }
            ENDCG
        }
    }
}
```

#### Blinn-Phong式高光反射光照模型

> 来历

Blinn-Phong式高光反射光照模型是Blinn-Phong光照模型的一部分，由Jim Blinn提出，主要用于计算高光反射颜色。

> 原理

Blinn-Phong是对Phong的一种改进，它不再使用反射向量计算镜面反射，而是使用半角向量来进行计算，半角向量为视角方向和灯光方向的角平分线方向。

Blinn-Phong认为高光反射的颜色和顶点法线向量以及半角向量夹角的余弦成正比，并且通过对余弦值取n次幂来表示光泽度（或反光度）。

> 公式

$$
高光反射光照颜色=光源颜色\times 材质高光反射颜色\times max(0,标准化后顶点法线方向向量\cdot 标准化后半角向量方向向量)^n
$$

> Phong和Blinn-Phong的区别

1. 高光反射：Blinn-Phong模型的高光通常会产生相对均匀的高光散射，这会使物体看起来光滑而均匀；Phong模型的高光可能会呈现更为锐利的高光散射，因为它基于观察者和光源之间的夹角，这可能导致一些区域看起来特别亮，而另一些区域则非常暗。
2. 高光锐度：Blinn-Phong模型的高光通常具有较广的散射角，因此看起来不那么锐利；Phong模型的高光可能会更加锐利，特别是在观察者和光源夹角较小时，可能表现为小而亮的点。
3. 光滑度和表面纹理：Blinn-Phong模型通常更适合表现光滑的表面，因为它考虑了表面围观凹凸之间的相互作用，使得光照在表面上更加均匀分布；Phong模型有时更适合表现具有粗糙表面纹理的物体，因为它的高光散射可能会使纹理和细节更加突出。
4. 镜面高光大小：Blinn-Phong模型通常产生的镜面高光相对较大，但均匀分布；Phong模型可能会产生较小且锐利的镜面高光。

在性能上，Blinn-Phong模型通常比Phong模型计算更快。

#### Blinn-Phong光照模型

> 来历

Blinn-Phong光照模型是由Jim Blinn在Phong光照模型的基础上进行修改提出的，它和Phong一样是一个经验模型，并不符合真实世界中的光照现象，它们只是看起来正确。

> 原理

Blinn-Phong和Phong光照模型一样，认为物体表面反射光线是由三部分组成的：环境光、漫反射光、镜面反射光。

> 公式

$$
物体表面光照颜色=环境光颜色+漫反射光颜色+高光反射光颜色
$$

> 逐顶点

```C
Shader "Custom/BlinnPhong"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.01, 100)) = 32
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos: SV_POSITION;
                fixed4 color: COLOR;
            };

            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _Shininess;

            fixed4 lambert(in float3 normal) 
            {
                float3 lightDir = normalize(_WorldSpaceLightPos0);
                float3 ndotl = dot(normal, lightDir);
                return _MainColor * _LightColor0 * fixed4(saturate(ndotl), 1);
            }

            fixed4 blinnPhong(in float4 vertex, in float3 normal)
            {
                float3 viewDir = normalize(UnityWorldSpaceViewDir(vertex));
                float3 lightDir = normalize(_WorldSpaceLightPos0);
                float3 halfDir = normalize(viewDir + lightDir);
                float3 ndoth = dot(normal, halfDir);
                ndoth = saturate(ndoth);
                return _SpecularColor * _LightColor0 * fixed4(pow(ndoth, _Shininess), 1);
            }

            v2f vert (appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                float4 worldVertex = mul(unity_ObjectToWorld, v.vertex);
                fixed4 diffuse = lambert(v.normal);
                fixed4 specular = blinnPhong(worldVertex, v.normal);
                fixed4 color = unity_AmbientSky + diffuse + specular;
                o.color = color;
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                return i.color;
            }
            ENDCG
        }
    }
}
```

> 逐片元

```C
Shader "Custom/BlinnPhongF"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.01, 100)) = 32
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos: SV_POSITION;
                fixed4 vertex: TEXCOORD;
                float3 normal: NORMAL;
            };

            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _Shininess;

            fixed4 lambert(in float3 normal) 
            {
                float3 lightDir = normalize(_WorldSpaceLightPos0);
                float3 ndotl = dot(normal, lightDir);
                return _MainColor * _LightColor0 * fixed4(saturate(ndotl), 1);
            }

            fixed4 blinnPhong(in float4 vertex, in float3 normal)
            {
                float3 viewDir = normalize(UnityWorldSpaceViewDir(vertex));
                float3 lightDir = normalize(_WorldSpaceLightPos0);
                float3 halfDir = normalize(viewDir + lightDir);
                float3 ndoth = dot(normal, halfDir);
                ndoth = saturate(ndoth);
                return _SpecularColor * _LightColor0 * fixed4(pow(ndoth, _Shininess), 1);
            }

            v2f vert (appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.vertex = mul(unity_ObjectToWorld, v.vertex);
                o.normal = v.normal;
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 diffuse = lambert(i.normal);
                fixed4 specular = blinnPhong(i.vertex, i.normal);
                fixed4 color = unity_AmbientSky + diffuse + specular;
                return color;
            }
            ENDCG
        }
    }
}
```

### 为什么逐片元比逐顶点平滑

#### 为何逐顶点光照比较粗糙

在顶点着色器中进行颜色计算，只是计算了模型顶点位置的颜色信息。

在光栅化阶段的片元着色器处理之前，会对三角形围着的片元颜色进行插值运算，再传递给片元着色器。

也就是说顶点之间的片元颜色信息，其实都不是使用该位置的相关信息计算的，而是粗暴的插值计算，计算颜色的平均值，那么自然呈现出来的效果是比较粗糙的。

#### 为何逐片元光照更平滑

片元着色器中传入的参数结构虽然和顶点着色器中返回的一样，但是里面的数据是不同的。

片元着色器中传入的数据，都是为每一个片元进行专门插值计算后的结果，因此我们利用这些数据再次进行光照计算，自然更加的平滑真实。

也就是说，在片元着色器中进行颜色相关计算时，能够为每一个像素点，基于它的相关数据单独进行颜色计算，而不是直接利用插值颜色，自然最终的表现效果会更加的平滑真实。

## 纹理

### 纹理概念

模型是由骨骼、三角面片和纹理组成的。

纹理的主要作用就是使用一张图片来控制模型的外观，使用纹理映射技术，将图片和模型联系起来，让模型能够呈现出图片中的颜色表现。

> 纹理映射

纹理映射是计算机图形学中的一种技术，用来将图像（纹理）映射到三维模型的表面，从而赋予模型更加真实和细致的外观，这个过程实际上是将二维图像映射到三维空间的过程。

#### 如何进行纹理映射

在建模软件中，美术同学会利用纹理展开技术把纹理映射坐标存储在每个顶点上，模型表面的顶点都与纹理坐标相关联，纹理坐标通常使用二维坐标系统（称为UV坐标，U表示水平轴，V表示垂直轴）。

#### UV坐标对于Shader开发的意义

在进行Shader开发时，可以在顶点着色器中获取模型中UV坐标的数据。

有了UV坐标，只要有正确的纹理贴图，就可以用顶点的UV坐标从图片中取出映射的颜色用于渲染，模型便可以呈现出颜色效果。

#### UV坐标的注意事项

1. 纹理坐标的横轴和纵轴的取值是被归一化过的，在[0,1]范围中，主要目的是为了适应不同大小的纹理图片。
2. 只有顶点中记录了UV坐标，因此在数据传递进片元着色器之前，UV坐标会在中间阶段进行插值运算，也就是每个片元中得到的UV坐标大部分都是插值计算的结果，因此可以得到图片中非顶点位置的颜色。

### 单张纹理

#### 纹理颜色采样

> 完成Shader文件基本结构

```C
Shader "Custom/Lesson48"
{
    Properties
    {
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            v2f_img vert (appdata_base v)
            {
                v2f_img o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord.xy;
                return o;
            }

            fixed4 frag (v2f_img i) : SV_Target
            {
                return fixed4(1,1,1,1);
            }
            ENDCG
        }
    }
}
```

> 纹理属性和CG成员变量声明

CG中映射ShaderLab中的纹理属性，需要有两个成员变量。一个用于映射纹理颜色数据，一个用于映射纹理缩放平移数据。

ShaderLab中的属性->图片属性（2D）：用于利用UV坐标提取其中颜色。

CG中用于映射属性的成员变量->sampler2D：用于映射纹理图片；float4：用于映射纹理图片的缩放和平移，固定命名方式为`纹理名_ST`。

> 用缩放平移参数参与uv值计算

1. 如何获取模型中携带的uv信息？
	在顶点着色器中，可以利用`TEXCOORD`语义来获取模型中的纹理坐标信息。xy是纹理坐标的水平和垂直坐标，zw是纹理携带的一些额外信息，例如深度值。
2. 如何计算
	先缩放（乘法），后平移（加法），公式为`纹理坐标.xy * 纹理名_ST.xy + 纹理名_ST.wz`，或使用内置函数`TRANSFORM_TEX(纹理坐标变量，纹理变量)`。

```C
Shader "Custom/Lesson48"
{
    Properties
    {
        _MainTex ("MainTex", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            sampler2D _MainTex;
            float4 _MainTex_ST;

            v2f_img vert (appdata_base v)
            {
                v2f_img o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw;
                // 另一种计算方式
                // o.uv = TRANSFORM_TEX(v.texcoord.xy, _MainTex);
                return o;
            }

            fixed4 frag (v2f_img i) : SV_Target
            {
                fixed4 color = tex2D(_MainTex, i.uv);
                return color;
            }
            ENDCG
        }
    }
}
```

#### 纹理结合光照模型

```C
Shader "Custom/Lesson50"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.01, 100)) = 10
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
                float4 worldVertex : TEXCOORD1;
                float3 worldNormal: NORMAL;
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;
            float4 _MainColor;
            float4 _SpecularColor;
            float _Shininess;

            v2f vert (appdata_base v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.texcoord.xy, _MainTex);                
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);
                o.worldNormal = UnityObjectToWorldNormal(v.normal);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                float4 texColor = tex2D(_MainTex, i.uv);

                // Lambert
                float3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));
                float3 ndotl = dot(i.worldNormal, lightDir);
                fixed3 diffuse = _LightColor0.rgb * texColor.rgb * saturate(ndotl);

                // Blinn-Phong
                float3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldVertex));
                float3 halfDir = normalize(lightDir + viewDir);
                float3 ndoth = dot(i.worldNormal, halfDir);
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(ndoth), _Shininess);

                // Final color
                fixed3 finalColor = unity_AmbientSky.rgb * texColor + diffuse + specular;
                return fixed4(finalColor, 1);
            }
            ENDCG
        }
    }
}
```

### 凹凸纹理

纹理除了用来映射颜色之外，还可以进行凹凸映射。

凹凸映射的目的是使用一张纹理来修改模型表面的法线，让模型在不增加顶点的情况下，使模型看起来同样充满细节（有凹凸效果），达到视觉上的“欺骗”。

原理：光照的计算都会利用法线参与计算，决定最终的颜色表现效果。在计算“假”凹凸面时，使用“真”凹凸面的法线参与计算，呈现出来的效果可以以假乱真。

凹凸映射有两种主流的方式：

1. 高度纹理贴图
2. 法线纹理贴图

#### 基本概念

##### 高度纹理贴图

高度纹理贴图一般简称高度图，它存储了模型表面上每个点的高度信息。

通常使用灰度图像，其中不同灰度值表示不同亮度。较亮区域通常对于较高的点，较暗区域对应较低的点。

> 存储规则

图片中某一个像素点的RGB值是相同的，都表示高度值，A值一般情况下为1。高度值范围一般为0~1，0代表最低，1代表最高。

> 优点

可以通过高度图很明确的知道模型表面的凹凸情况。

> 缺点

无法在Shader中直接得到模型表面点的法线信息，而是需要通过额外的计算得到，因此会增加性能消耗，所以几乎很少使用它。

##### 法线纹理贴图

法线纹理贴图一般简称法线贴图或法线纹理，它存储了模型表面上每个点的法线方向。

> 存储规则

图片中的RGB值分别存储法线的X、Y、Z分量值，A值可以用于存储其他信息，比如材质光滑度等。

> 优点

从法线贴图中取出的数据就是法线信息，简单处理后就可以直接参与光照计算，性能更好。

> 缺点

无法直观地看出模型表面的凹凸情况。

##### 法线纹理贴图存储规则

> 读取分量数据的规则

由于法线XYZ分量范围是`[-1,1]`，而像素RGB分量范围是`[0,1]`，因此我们需要做一个映射计算。

$$
\begin{array}\
像素分量=(法线分量+1)\div 2 \\
法线分量=像素分量\times 2 - 1
\end{array}
$$

> 两种法线贴图的存储方式

法线纹理贴图中主要存储法线信息，而法线信息其实就是方向向量，而方向向量就得有相对坐标系，因此法线贴图的存储方式按相对坐标系划分有两种：

1. 基于模型空间的法线纹理
2. 基于切线空间的法线纹理

**基于模型空间的法线纹理**

模型数据中自带的法线数据，是定义在模型空间中的，因此最直接的存储方式就是存储基于模型空间的法线信息。

基于模型空间的法线纹理一般是五颜六色的，这种法线纹理贴图数据取出来可以直接参与光照计算。

**基于切线空间的法线纹理**

虽然基于模型空间的法线纹理贴图看起来很符合计算需求，但是实际开发时，美术同学一般给我们的是基于切线空间的法线贴图。

在切线空间中：

- 原点：顶点本身
- X轴：顶点切线
- Z轴：法线方向
- Y轴：X和Z的叉乘结果，也被称为副切线

> 切线空间下的法线贴图是蓝色的原因

在切线空间下，如果该顶点的法线不变化（不需要“凹凸感”），那么它的坐标是`(0,0,1)`。因为在切线空间下，Z轴就是原法线方向。而法线`(0,0,1)`映射到像素后是`(0.5,0.5,1)`，是浅蓝色，所以法线贴图存在大面积的蓝色。

因为大部分顶点的法线和模型本身法线是一致的，只有凹凸部分的颜色才会有些许差异。

这种法线纹理贴图数据取出来后，需要先进行坐标空间变换，再参与计算。

**为什么要使用切线空间下的法线贴图**

1. 可以用于不同模型，方便处理模型变形：模型空间下的法线贴图不能用于其他模型。
2. 可以复用：一个砖块，6个面都是一样的，可以只用一张法线贴图计算6个面。
3. 可以压缩：可以只存储两个轴的分量。
4. 方便制作UV动画：UV坐标改变可以实现凹凸移动效果，如果是模型空间下的法线贴图，表现会有纹理。

#### 法线贴图的计算方式

##### 两种主流计算方式

1. 在切线空间下进行光照计算，需要把光照方向、视角方向变换到切线空间下。
2. 在世界空间下进行光照计算，需要把法线方向变换到世界空间下。

##### 各自的优缺点

> 效率

在切线空间下计算效率更高，因为可以在顶点着色器中就完成对光照、视角方向的矩阵变换，计算量相对较小。

在世界空间下计算效率较低，因为需要对法线贴图进行采样，所以变换过程必须在片元着色器中实现，需要在片元着色器中对法线进行矩阵变换。

> 全局效果

在切线空间中计算，对全局效果的表现可能会不够准确（镜面反射、环境映射效果）。

在世界空间中计算，对全局效果的表现更加准确，且更容易应用。

##### 在切线空间下计算

在切线空间下进行光照计算，需要把光照方向、视角方向变换到切线空间下参与计算。

关键是计算出模型空间到切线空间的变换矩阵，也是切线空间到模型空间的逆矩阵。

由于我们主要用变换矩阵来进行矢量的变换而非点的变换，因此只需要计算3×3的矩阵即可。

已知切线、法线，副切线为切线和法线的叉积，而三个轴为相互垂直的单位向量，因此可以推出从切线空间变换到模型空间的矩阵为正交矩阵，其逆矩阵就是其转置矩阵。

##### 在世界空间下计算

在世界空间下进行光照计算，需要把法线方向变换到世界空间下参与计算。

关键是计算出切线空间到世界空间的变换矩阵。

- 世界空间法线：`UnityObjectToWorldNormal(模型空间法线)`
- 世界空间切线：`UnityObjectToWorldDir(模型空间切线)`
- 世界空间副切线：法线和切线叉乘

由三个向量组成最终的切线空间到世界空间的变换矩阵即可。

##### 关键知识点补充

> 模型空间下的切线数据

模型空间下的切线数据是float4类型的，其中的w表示副切线的方向。

用法线和切线叉乘得到的副切线方向可能有两个，用切线数据中的w与之相乘就可以确定副切线方向。

> Unity当中的法线纹理类型

当我们把纹理类型设置为Normal map时，我们可以使用Unity提供的内置函数`UnpackNormal`来得到正确的法线方向。

该函数内部不仅可以进行法线分量和像素分量的转换计算，还会进行解压运算（Unity根据不同平台对法线纹理进行压缩）。

> 控制凹凸程度

法线纹理属性一般命名为`_BumpMap`（凸块贴图），我们还会声明一个名为`_BumpScale`（凸块缩放）的float属性，用于控制凹凸程度。

当缩放为0时，表示没有法线效果，为1时，表示使用法线贴图中原始的法线信息。

可以根据需求调整，来达到视觉上令人满意的效果。

> 高度纹理设置

如果使用的凹凸纹理不是法线纹理，而是高度纹理，可以进行如下设置。

将图片类型设置为`Normal map`，勾选`Create from Grayscale`，这样就可以把高度纹理当成切线空间下的法线纹理处理了。

#### 切线空间下计算法线贴图

```C
Shader "Custom/Lesson51"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "" {}
        _BumpMap ("Normal Map", 2D) = "" {}
        _BumpScale ("BumpScale", Range(0, 1)) = 1
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.1, 20)) = 2
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float4 uv : TEXCOORD0;
                float3 lightDir: TEXCOORD1;
                float3 viewDir: TEXCOORD2;
            };

            float4 _MainColor;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpMap;
            float4 _BumpMap_ST;
            float _BumpScale;
            float4 _SpecularColor;
            float _Shininess;

            v2f vert (appdata_full v)
            {
                v2f o;

                o.pos = UnityObjectToClipPos(v.vertex);

                // 计算纹理缩放偏移
                o.uv.xy = TRANSFORM_TEX(v.texcoord.xy, _MainTex);
                o.uv.zw = TRANSFORM_TEX(v.texcoord.xy, _BumpMap);

                // 计算副切线
                float3 binormal = normalize(cross(v.normal, v.tangent)) * v.tangent.w;
                // 获得模型空间到切线空间的变换矩阵
                float3x3 tbn = float3x3(v.tangent.xyz, binormal, v.normal);
                
                // 获得切线空间下的光照方向和视线方向
                o.lightDir = mul(tbn, ObjSpaceLightDir(v.vertex));
                o.viewDir = mul(tbn, ObjSpaceViewDir(v.vertex));

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 通过纹理采样函数，取出法线贴图中的数据
                float4 packedNormal = tex2D(_BumpMap, i.uv.zw);
                // 对法线贴图数据进行逆运算和解压，得到切线空间下的法线
                float3 tangentNormal = UnpackNormal(packedNormal);
                // 乘以凹凸程度系数
                tangentNormal *= _BumpScale;

                // 叠加颜色纹理和漫反射颜色
                fixed3 texColor = tex2D(_MainTex, i.uv.xy) * _MainColor;

                // Lambert
                float3 ndotl = dot(tangentNormal, i.lightDir);
                fixed3 diffuse = _LightColor0.rgb * texColor.rgb * saturate(ndotl);

                // Blinn-Phong
                float3 halfDir = normalize(i.lightDir + i.viewDir);
                float3 hdotn = dot(halfDir, tangentNormal);
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(hdotn), _Shininess);

                // 最终颜色
                fixed3 color = unity_AmbientSky * texColor + diffuse + specular;

                return fixed4(color, 1);
            }
            ENDCG
        }
    }
}
```

#### 世界空间下计算法线贴图

```C
Shader "Custom/Lesson52"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "" {}
        _BumpMap ("Normal Map", 2D) = "" {}
        _BumpScale ("BumpScale", Range(0, 1)) = 1
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.1, 20)) = 2
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float4 uv : TEXCOORD0;
                float4 vertex : TEXCOORD1;
                float3x3 tangentToWorld: TEXCOORD2;
            };

            float4 _MainColor;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpMap;
            float4 _BumpMap_ST;
            float _BumpScale;
            float4 _SpecularColor;
            float _Shininess;

            v2f vert (appdata_full v)
            {
                v2f o;

                o.pos = UnityObjectToClipPos(v.vertex);

                o.uv.xy = TRANSFORM_TEX(v.texcoord.xy, _MainTex);
                o.uv.zw = TRANSFORM_TEX(v.texcoord.xy, _BumpMap);

                o.vertex = mul(unity_ObjectToWorld, v.vertex);

                float3 worldNormal = UnityObjectToWorldNormal(v.normal);
                float3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz);
                float3 binormal = cross(worldNormal, worldTangent) * v.tangent.w;
                
                o.tangentToWorld = float3x3
                (
                    worldTangent.x, binormal.x, worldNormal.x,
                    worldTangent.y, binormal.y, worldNormal.y,
                    worldTangent.z, binormal.z, worldNormal.z
                );

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                float4 packedNormal = tex2D(_BumpMap, i.uv.zw);
                float3 tangentNormal = UnpackNormal(packedNormal);

                tangentNormal *= _BumpScale;
                tangentNormal = mul(i.tangentToWorld, tangentNormal);

                // Albedo
                fixed3 albedo = tex2D(_MainTex, i.uv.xy) * _MainColor.rgb;

                // Lambert
                float3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));
                float3 ndotl = dot(tangentNormal, lightDir);
                fixed3 diffuse = _LightColor0.rgb * albedo * saturate(ndotl);

                // Blinn-Phong
                float3 viewDir = normalize(UnityWorldSpaceViewDir(i.vertex));
                float3 halfDir = normalize(lightDir + viewDir);
                float3 ndoth = dot(tangentNormal, halfDir);
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(ndoth), _Shininess);

                // Final
                fixed3 color = unity_AmbientSky * albedo + diffuse + specular;

                return fixed4(color, 1);
            }
            ENDCG
        }
    }
}
```

避免法线乘以凹凸系数后影响光照的计算，可以使用一种算法。

首先只让法线中的xy分量乘以凹凸系数，然后手动计算z分量，让法线保持为单位向量。

```C
tangentNormal.xy *= _BumpScale;
tangentNormal.z = sqrt(1 - dot(tangentNormal.xy, tangentNormal.xy));
```

> 提高性能

目前我们在v2f结构体中，世界坐标顶点位置和变换矩阵使用了float3和float3x3的两个变量来存储。

但是在很多世界空间下计算法线贴图的shader中，往往会使用3个float4类型的变量来存储它们。

因为这样写，在很多情况下可以提高性能，可以更好地和GPU的硬件架构匹配。现代GPU通常会以4分量的向量为基本单位进行并行计算，所以float4类型的寄存器是非常高效的，而float3x3的矩阵相对来说需要更多的寄存器和指令来表示和计算。

修改部分如下。

```C
struct v2f
{
	float4 pos : SV_POSITION;
	float4 uv : TEXCOORD0;
	float4 t2w0: TEXCOORD1;
	float4 t2w1: TEXCOORD2;
	float4 t2w2: TEXCOORD3;
};

v2f vert (appdata_full v)
{
	v2f o;
	...
	o.t2w0 = float4(worldTangent.x, binormal.x, worldNormal.x, vertex.x);
	o.t2w1 = float4(worldTangent.y, binormal.y, worldNormal.y, vertex.y);
	o.t2w2 = float4(worldTangent.z, binormal.z, worldNormal.z, vertex.z);

	return o;
}

fixed4 frag (v2f i) : SV_Target
{
	...
	float3 vertex = float3(i.t2w0.w, i.t2w1.w, i.t2w2.w);
	
	tangentNormal = float3
	(
		dot(i.t2w0.xyz, tangentNormal),
		dot(i.t2w1.xyz, tangentNormal),
		dot(i.t2w2.xyz, tangentNormal)
	);
	
	...
	return fixed4(color, 1);
}
```

### 渐变纹理

渐变纹理就是用于控制漫反射光照结果的一种存储数据的方式，它的主要作用是让游戏中的对象具有插画卡通风格。

使用渐变纹理可以保证物体的轮廓线相比之前使用的传统漫反射光照更明显，而且还能提供多种色调变化，让模型更具卡通感。

> 基本原理

在计算漫反射时，利用半兰伯特光照模型公式中后半部分得到一个[0,1]区间的值，将这个值作为uv坐标中的uv值，从渐变纹理中取出颜色与公式前面的部分进行颜色叠加，最终得到漫反射光照颜色。

也就是说，决定漫反射明暗的不再是由0~1这个值决定，而是由渐变纹理中取出的颜色进行叠加达到最终效果。

> 基础实现

```C
Shader "Custom/Lesson53"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _GradientTex ("Gradient Texture", 2D) = "" {}
        _SpecularColor ("Specular", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.1, 20)) = 0.5
    }

    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos: SV_POSITION;
                float4 vertex : TEXCOORD1;
                float3 normal: NORMAL;
            };

            fixed4 _MainColor;
            sampler2D _GradientTex;
            float4 _GradientTex_ST;
            fixed4 _SpecularColor;
            float _Shininess;

            v2f vert (appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.vertex = mul(unity_ObjectToWorld, v.vertex);
                o.normal = UnityObjectToWorldNormal(v.normal);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                float3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));
                float3 viewDir = normalize(UnityWorldSpaceViewDir(i.vertex));
                float3 halfDir = normalize(lightDir + viewDir);

                // Half-Lambert
                fixed halfLambert = dot(i.normal, lightDir) * 0.5 + 0.5;
                fixed4 gradient = tex2D(_GradientTex, fixed2(halfLambert, halfLambert));
                fixed4 diffuse = _LightColor0 * _MainColor * gradient;

                // Blinn-Phong
                float hdoth = dot(i.normal, halfDir);
                fixed4 specular = _LightColor0 * _SpecularColor * pow(saturate(hdoth), _Shininess);

                // Final
                fixed4 color = unity_AmbientSky + diffuse + specular;

                return color;
            }
            ENDCG
        }
    }
}
```

> 综合实现(基于凹凸纹理Shader)

```C
Shader "Custom/Lesson54"
{
    Properties
    {
        _MainColor ("Main Color", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "" {}
        _RampTex ("Ramp Texture", 2D) = "" {}
        _BumpMap ("Normal Map", 2D) = "" {}
        _BumpScale ("BumpScale", Range(0, 1)) = 1
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)
        _Shininess ("Shininess", Range(0.1, 20)) = 2
    }
    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float4 uv : TEXCOORD0;
                float4 t2w0: TEXCOORD1;
                float4 t2w1: TEXCOORD2;
                float4 t2w2: TEXCOORD3;
            };

            float4 _MainColor;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _RampTex;
            float4 _RampTex_ST;
            sampler2D _BumpMap;
            float4 _BumpMap_ST;
            float _BumpScale;
            float4 _SpecularColor;
            float _Shininess;

            v2f vert (appdata_full v)
            {
                v2f o;

                o.pos = UnityObjectToClipPos(v.vertex);

                o.uv.xy = TRANSFORM_TEX(v.texcoord.xy, _MainTex);
                o.uv.zw = TRANSFORM_TEX(v.texcoord.xy, _BumpMap);

                float3 vertex = mul(unity_ObjectToWorld, v.vertex);

                float3 worldNormal = UnityObjectToWorldNormal(v.normal);
                float3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz);
                float3 binormal = cross(worldNormal, worldTangent) * v.tangent.w;

                o.t2w0 = float4(worldTangent.x, binormal.x, worldNormal.x, vertex.x);
                o.t2w1 = float4(worldTangent.y, binormal.y, worldNormal.y, vertex.y);
                o.t2w2 = float4(worldTangent.z, binormal.z, worldNormal.z, vertex.z);

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                float3 vertex = float3(i.t2w0.w, i.t2w1.w, i.t2w2.w);

                float4 packedNormal = tex2D(_BumpMap, i.uv.zw);
                float3 tangentNormal = UnpackNormal(packedNormal);

                // 避免法线缩放后影响光照，只对xy分量进行缩放
                tangentNormal.xy *= _BumpScale;
                // 手动计算z分量，使法线保持为单位向量
                tangentNormal.z = sqrt(1 - dot(tangentNormal.xy, tangentNormal.xy));

                tangentNormal = float3
                (
                    dot(i.t2w0.xyz, tangentNormal),
                    dot(i.t2w1.xyz, tangentNormal),
                    dot(i.t2w2.xyz, tangentNormal)
                );

                // Albedo
                fixed3 albedo = tex2D(_MainTex, i.uv.xy) * _MainColor.rgb;

                // Lambert
                float3 lightDir = normalize(UnityWorldSpaceLightDir(vertex));
                float halfLambert = dot(tangentNormal, lightDir) * 0.5 + 0.5;
                fixed3 gradient = tex2D(_RampTex, fixed2(halfLambert, halfLambert));
                fixed3 diffuse = _LightColor0.rgb * albedo * gradient;

                // Blinn-Phong
                float3 viewDir = normalize(UnityWorldSpaceViewDir(vertex));
                float3 halfDir = normalize(lightDir + viewDir);
                float ndoth = dot(tangentNormal, halfDir);
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(ndoth), _Shininess);

                // Final
                fixed3 color = unity_AmbientSky * albedo + diffuse + specular;

                return fixed4(color, 1);
            }
            ENDCG
        }
    }
}
```

### 遮罩纹理

#### 纹理图片存储的数据

纹理中可以存储的数据可以是：

- 颜色（单张纹理）
- 法线、高度（凹凸纹理）
- 漫反射光的强度色（渐变纹理）

一张图片中主要存储的就是像素点数据，常见情况下，一个像素点存储的RGBA值通常占据4个字节，每个颜色通道通常使用1个字节/（0~255）表示。

因此，完全可以在图片数据中存储非颜色的数据。

比如凹凸纹理的图片数据实际上存储的是：

- 法线数据：RGB分别存储模型表面点的法线数据的XYZ分量，A一般为1。
- 高度数据：RGB存储的值相同，为模型表面点的高度数据，A一般为1。

#### 遮罩纹理是用来做什么的

遮罩纹理通常用于控制或限制某些效果的显示范围，它允许我们可以保护某些区域，使它们免于某些修改。

一般情况下，遮罩纹理也会是一张灰度图，其中的RGB值会是相同的，我们利用它存储的值参与到光照、透明度、特效等等相关的计算中，来让指定区域达到我们想要的效果。

- 光照：指定某些区域受光影响的程度
- 透明度：指定某些区域透明的程度
- 特效：指定某些区域出现特效

#### 高光遮罩纹理的基本原理

1. 从纹理中取出对应的遮罩掩码值（RGB值都可以）
2. 用该掩码值乘以遮罩系数得到遮罩值
3. 用遮罩值乘以高光反射颜色

最终呈现出来的高光反射表现就会受到高光遮罩纹理和遮罩系数的影响，从而表现出最终效果。

#### 高光遮罩纹理综合实现

```C
Shader "Custom/Lesson55"  
{  
    Properties  
    {  
        _MainColor ("Main Color", Color) = (1,1,1,1)  
        _MainTex ("Texture", 2D) = "" {}  
        _BumpMap ("Normal Map", 2D) = "" {}  
        _BumpScale ("BumpScale", Range(0, 1)) = 1  
        _SpecularMask ("Specular Mask", 2D) = "" {}  
        _MaskScale ("MaskScale", Float) = 1  
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)  
        _Shininess ("Shininess", Range(0.1, 20)) = 2  
    }  
    SubShader  
    {  
        Pass  
        {  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
  
            struct v2f  
            {  
                float4 pos : SV_POSITION;  
                float4 uv : TEXCOORD0;  
                float3 lightDir: TEXCOORD1;  
                float3 viewDir: TEXCOORD2;  
            };  
            float4 _MainColor;  
            sampler2D _MainTex;  
            float4 _MainTex_ST;  
            sampler2D _BumpMap;  
            float4 _BumpMap_ST;  
            float _BumpScale;  
            sampler2D _SpecularMask;  
            float4 _SpecularMask_ST;  
            float _MaskScale;  
            float4 _SpecularColor;  
            float _Shininess;  
  
            v2f vert(appdata_full v)  
            {                v2f o;  
  
                o.pos = UnityObjectToClipPos(v.vertex);  
  
                // 计算纹理缩放偏移  
                o.uv.xy = TRANSFORM_TEX(v.texcoord.xy, _MainTex);  
                o.uv.zw = TRANSFORM_TEX(v.texcoord.xy, _BumpMap);  
  
                // 计算副切线  
                float3 binormal = normalize(cross(v.normal, v.tangent)) * v.tangent.w;  
                // 获得模型空间到切线空间的变换矩阵  
                float3x3 tbn = float3x3(v.tangent.xyz, binormal, v.normal);  
  
                // 获得切线空间下的光照方向和视线方向  
                o.lightDir = mul(tbn, ObjSpaceLightDir(v.vertex));  
                o.viewDir = mul(tbn, ObjSpaceViewDir(v.vertex));  
  
                return o;  
            }  
            fixed4 frag(v2f i) : SV_Target  
            {  
                // 通过纹理采样函数，取出法线贴图中的数据  
                float4 packedNormal = tex2D(_BumpMap, i.uv.zw);  
                // 对法线贴图数据进行逆运算和解压，得到切线空间下的法线  
                float3 tangentNormal = UnpackNormal(packedNormal);  
                // 乘以凹凸程度系数  
                tangentNormal *= _BumpScale;  
  
                // 叠加颜色纹理和漫反射颜色  
                fixed3 texColor = tex2D(_MainTex, i.uv.xy) * _MainColor;  
  
                // Lambert  
                float ndotl = dot(tangentNormal, i.lightDir);  
                fixed3 diffuse = _LightColor0.rgb * texColor.rgb * saturate(ndotl);  
  
                // Blinn-Phong  
                float3 halfDir = normalize(i.lightDir + i.viewDir);  
                float mask = tex2D(_SpecularMask, i.uv.xy).r * _MaskScale;  
                float hdotn = dot(halfDir, tangentNormal);  
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(hdotn * mask), _Shininess);  
  
                // 最终颜色  
                fixed3 color = unity_AmbientSky * texColor + diffuse + specular;  
  
                return fixed4(color, 1);  
            }            ENDCG  
        }  
    }
}
```

#### 遮罩纹理中的RGBA值

可以在遮罩纹理中的存储更多的数据，比如透明遮罩、特效遮罩等等，而不只是高光遮罩数据，避免空间的浪费。

甚至可以用n张遮挡纹理存储4xn个会参与每个片元渲染计算的值。

## 透明

### 渲染顺序的重要性

#### 深度测试和深度写入带来的好处

有了深度测试和深度写入发挥作用，我们就可以不用关心不透明物体的渲染顺序。

在处理透明混合时，需要关闭深度写入。

#### 透明混合为什么要关闭深度写入

在图形学中模拟现实世界的半透明效果是通过将多个颜色混合计算呈现出来的，关闭透明物体的深度写入，可以让其不会导致被遮挡物体无法通过深度测试而无法进行颜色混合。

#### 关闭深度写入带来的问题

若关闭了深度写入，物体的渲染顺序就变得非常重要了，它会影响最终呈现的效果。

假设A物体（半透明）在B物体（不透明）前面：若先渲染A，再渲染B，就只能看到B的颜色；若先渲染B，再渲染A，就可以得到正确的半透明效果。

因此，在透明物体和不透明物体之间，要想得到正确的半透明效果，就必须先渲染不透明物体，再渲染透明物体。

假设A和B都是半透明物体，A还是在B的前面：若先渲染A，再渲染B，则会得到错误的半透明效果（B的颜色在前面）；若先渲染B，再渲染A，则会得到正确的半透明效果。

因此，在两个不透明物体之间，要想得到正确的半透明效果，也需要保证渲染顺序的正确。

#### 如何解决渲染顺序带来的问题

为了解决渲染顺序带来的问题，大部分的渲染引擎采取的方式都是先对场景上的物体进行排序，然后再渲染。常规方法为：

1. 先渲染所有不透明物体，开启深度测试和深度写入。
2. 将半透明物体按照摄像机的距离进行排序，然后从后往前去渲染，开启深度测试，关闭深度写入。

在对半透明物体进行排序时，是基于物体的，而不是基于片元的。所以在一些特殊情况下，还是会出现渲染上的错误。解决方法有：

1. 忍一忍，接收部分细节的瑕疵。
2. 从模型入手，拆分模型。
3. 使用开启深度写入的半透明效果来模拟半透明（性能消耗高）。
### 设置深度写入和渲染队列

深度写入默认是开启的，需要使用`ZWrite Off`命令来关闭，可以写在Pass或SubShader语句块中。

渲染队列是通过渲染标签来设置的，一般定义在SubShader语句块中。在实现透明效果时，主要使用`AlphaTest`和`Transparent`队列。

在使用渲染队列Queue时，一般会搭配忽视投影机IgnoreProjector和渲染类型RenderType一起使用。

- IgnoreProjector：投影机是Unity中的一种特殊光源，它用于在场景中投射纹理或简单的几何形状来模拟光照、阴影或其他视觉效果。
- RenderType：对着色器进行分类，用于着色器替换功能，摄像机上有对应API，可以指定渲染类型替换成别的着色器。

### 设置混合命令

#### 混合的基本原理

在渲染时，当片元通过了深度测试，就会进入混合流程中。

混合就是将源颜色（当前片元的颜色）和目标颜色（颜色缓冲区中的颜色）用相应的混合算法进行计算，得到一个新的颜色，并更新到颜色缓冲区中。

#### 混合的计算规则

假设源颜色为S，目标颜色为D，输出颜色为O，使用下面两个等式进行混合计算：

1. 计算RGB通道的混合等式：$O_{rgb}=源因子\times S_{rgb}+目标因子\times D_{rgb}$
2. 计算A通道的混合等式：$O_a=源透明因子\times S_a+目标透明因子\times D_a$

> 混合因子

混合命令的写法为`Blend 源因子 目标因子，源透明因子 目标透明因子`，如果省略后面的透明因子，则源透明因子和源因子相同，目标透明因子和目标因子相同。

ShaderLab提供的混合因子：[Unity - Manual: ShaderLab command: Blend](https://docs.unity.cn/2023.2/Documentation/Manual/SL-Blend.html)

> 混合操作

还可以使用`Blendop`命令来设置混合操作，改变混合的计算方式。

ShaderLab提供的混合操作：[Unity - Manual: ShaderLab command: BlendOp](https://docs.unity.cn/2023.2/Documentation/Manual/SL-BlendOp.html)

#### 常见的混合类型

![[Pasted image 20240717171532.png]]

### 透明效果实现

#### 透明度测试

> 透明测试是用来处理哪种透明需求的

在游戏开发中，对象的某些部位完全透明，而其他部分完全不透明，这种透明需求往往不需要半透明效果，相对比较极端，只有看得见和看不见之分。

> 透明测试的基本原理

通过一个阈值来决定哪些像素应该被保留，哪些应该被丢弃。

具体实现：片元携带的颜色信息中的透明度，不满足条件时，片元就会被舍弃，满足条件时，片元会按照不透明物体的处理方式来处理。

阈值判断方法：CG内置函数`clip`，当传入参数的任何一个分量为负数时，就会舍弃当前片元。其内部实现会使用`discard`指令，表示删除该片元，不再参与渲染。

> 实现

```C
Shader "Custom/AlphaTest"  
{  
    Properties  
    {  
        _MainTex ("Texture", 2D) = "white" {}  
        _MainColor ("Main Color", Color) = (1,1,1,1)  
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)  
        _Shininess ("Shininess", Range(0.01, 100)) = 10  
        _Cutoff("Cutoff", Range(0, 1)) = 0.2  
    }  
    SubShader  
    {  
        Tags  
        {  
            "Queue"="AlphaTest" "IgnoreProjector"="True" "RenderType"="TransparentCutout"  
        }  
        Pass  
        {  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
  
            struct v2f  
            {  
                float2 uv : TEXCOORD0;  
                float4 vertex : SV_POSITION;  
                float4 worldVertex : TEXCOORD1;  
                float3 worldNormal: NORMAL;  
            };  
            sampler2D _MainTex;  
            float4 _MainTex_ST;  
            float4 _MainColor;  
            float4 _SpecularColor;  
            float _Shininess;  
            fixed _Cutoff;  
  
            v2f vert(appdata_base v)  
            {                v2f o;  
                o.vertex = UnityObjectToClipPos(v.vertex);  
                o.uv = TRANSFORM_TEX(v.texcoord.xy, _MainTex);  
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);  
                o.worldNormal = UnityObjectToWorldNormal(v.normal);  
                return o;  
            }  
            fixed4 frag(v2f i) : SV_Target  
            {  
                float4 texColor = tex2D(_MainTex, i.uv);  
  
                // 剔除透明度小于阈值的片元  
                clip(texColor.a - _Cutoff);  
  
                // Lambert  
                float3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));  
                float3 ndotl = dot(i.worldNormal, lightDir);  
                fixed3 diffuse = _LightColor0.rgb * texColor.rgb * saturate(ndotl);  
  
                // Blinn-Phong  
                float3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldVertex));  
                float3 halfDir = normalize(lightDir + viewDir);  
                float3 ndoth = dot(i.worldNormal, halfDir);  
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(ndoth), _Shininess);  
  
                // Final color  
                fixed3 finalColor = unity_AmbientSky.rgb * texColor + diffuse + specular;  
                return fixed4(finalColor, 1);  
            }            ENDCG  
        }  
    }
}
```

#### 透明度混合

> 基本原理

关闭深度写入，开启混合，让片元颜色和颜色缓冲区中的颜色进行混合计算。

> 具体实现

采用半透明的混合因子进行混合，`Blend SrcAlpha OneMinusSrcAlpha`。

声明一个0~1区间的_AlphaScale，用于控制对象整体透明度。

```C
Shader "Custom/AlphaBlend"  
{  
    Properties  
    {  
        _MainTex ("Texture", 2D) = "white" {}  
        _MainColor ("Main Color", Color) = (1,1,1,1)  
        _SpecularColor ("Specular Color", Color) = (1,1,1,1)  
        _Shininess ("Shininess", Range(0.01, 100)) = 10  
        _AlphaScale ("Alpha Scale", Range(0, 1)) = 1  
    }  
    SubShader  
    {  
        Tags  
        {  
            "Queue"="Transparent" "IgnoreProjector"= "True" "RenderType"="Transparent"  
        }  
        Pass  
        {  
            ZWrite Off  
            Blend SrcAlpha OneMinusSrcAlpha  
            CGPROGRAM            #pragma vertex vert  
            #pragma fragment frag  
  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
  
            struct v2f  
            {  
                float2 uv : TEXCOORD0;  
                float4 vertex : SV_POSITION;  
                float4 worldVertex : TEXCOORD1;  
                float3 worldNormal: NORMAL;  
            };  
            sampler2D _MainTex;  
            float4 _MainTex_ST;  
            float4 _MainColor;  
            float4 _SpecularColor;  
            float _Shininess;  
            fixed _AlphaScale;  
  
            v2f vert(appdata_base v)  
            {                v2f o;  
                o.vertex = UnityObjectToClipPos(v.vertex);  
                o.uv = TRANSFORM_TEX(v.texcoord.xy, _MainTex);  
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);  
                o.worldNormal = UnityObjectToWorldNormal(v.normal);  
                return o;  
            }  
            fixed4 frag(v2f i) : SV_Target  
            {  
                float4 texColor = tex2D(_MainTex, i.uv);  
  
                // Lambert  
                float3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));  
                float3 ndotl = dot(i.worldNormal, lightDir);  
                fixed3 diffuse = _LightColor0.rgb * texColor.rgb * saturate(ndotl);  
  
                // Blinn-Phong  
                float3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldVertex));  
                float3 halfDir = normalize(lightDir + viewDir);  
                float3 ndoth = dot(i.worldNormal, halfDir);  
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(saturate(ndoth), _Shininess);  
  
                // Final color  
                fixed3 finalColor = unity_AmbientSky.rgb * texColor + diffuse + specular;  
                return fixed4(finalColor, texColor.a * _AlphaScale);  
            }            ENDCG  
        }  
    }
}
```

#### 开启深度写入的半透明效果

> 需求

对于本身结构比较复杂的模型，使用之前的透明度混合Shader会由于关闭了深度写入，而产生错误的渲染效果。

虽然我们可以通过拆分模型的方式来解决部分问题，但是对于一些结构复杂的模型，拆分模型的方式会增加工作量。

因此我们可以采用开启深度写入的半透明Shader来优化效果。

> 基本原理

使用两个Pass渲染通道来处理渲染逻辑：

1. 第一个Pass：开启深度写入，不输出颜色，目的是让该模型各片元的深度值能写入深度缓冲。
2. 第二个Pass：进行正常的透明度混合。

这样做的话，当执行第一个Pass时，会执行深度测试，并进行深度写入，如果此时该片元没有通过深度测试会直接丢弃，不会再执行第二个Pass。

对于同一个模型中处于屏幕同一位置的片元，会进行该位置的深度测试再决定渲染哪个片元。

> 如何做到不输出颜色

使用`ColorMask`渲染状态，其主要用于控制颜色分量是否写入到颜色缓冲区。

- Color Mask RGBA：表示写入颜色的RGBA通道。
- Color Mask 0：表示不写入颜色。
- Color Mask RB：表示只写入红色和蓝色通道。

> 注意

开启深度写入的半透明效果，模型内部之间不会有任何半透明效果，因为模型内部深度较大的片元会被丢弃。

由于有两个Pass渲染通道，会带来额外的性能开销。

#### 双面渲染的透明效果

> 需求

对于现实世界的半透明物体，我们不仅可以透过它看到其他物体，也可以看到物体自己的内部结构，但是之前实现的透明度测试和透明度混合的Shader，是无法看到模型内部结构的。

双面渲染的透明效果Shader就是来解决这个问题的，让我们不仅可以透过半透明物体看签到其他物体的样子，还可以看到自己的内部结构。

> 基本原理

默认情况下，Unity会自动剔除物体的背面，只渲染物体的正面，双面渲染的基本原理就是利用`Cull`指令来进行指定操作。

- Cull Back：背面剔除
- Cull Front：正面剔除
- Cull Off：不剔除

默认为背面剔除。

对于透明度测试Shader，由于它无需混合，直接关闭剔除即可。

对于透明度混合Shader，由于需要进行混合，需要使用两个Pass，一个用于渲染背面，一个用于渲染正面，两个Pass中除了剔除命令不同，其他代码和之前一致。

# Shader基础知识

## 光照和阴影

### 渲染路径

#### 概述

渲染路径是指在图形渲染过程中，图形引擎按照特定的步骤和顺序来处理场景中的几何、光照、材质等信息，最终生成屏幕上的图像的一种算法或策略。

它决定了图形引擎如何组织和执行渲染过程，以产生最终的视觉效果。

总而言之，渲染路径会影响光照处理，从而影响最终的渲染效果（光照、阴影等），渲染路径是有多种的。

> 渲染路径的种类和设置

渲染路径可以在Camera组件中进行修改，对应Rendering Path属性。在内置渲染管线中主要有三种：

1. Forward（前向渲染）：默认的标准的渲染方式，适用于相对简单的场景和较少的光源。
2. Deferred（延迟渲染）：可以处理较复杂的场景，有大量光源时可以提供更好的性能。
3. Legacy Vertex Lit（顶点照明渲染路径）：适用于较为简单的渲染方式，适用于u性能受限的场景，基本不会再使用。

注：当显卡不支持选定的渲染路径时，会自动选择一个较低精度的渲染路径，比如不支持延迟渲染路径时，会采用前向渲染路径。

> LightMode标签的作用

| 标签名                          | 描述                                                         |
| ---------------------------- | ---------------------------------------------------------- |
| Always                       | 不管使用哪种渲染路径，该Pass始终渲染，但不会计算任何光照                             |
| ForwardBase                  | 前向渲染路径的基础照明阶段执行的Pass，该Pass会计算环境光、重要的平行光、逐顶点/SH光源和Lightmaps |
| ForwardAdd                   | 前向渲染路径的额外照明阶段执行的Pass，该Pass会计算额外的逐像素光源，每个Pass对应一个光源         |
| Deferred                     | 延迟渲染路径中执行的Pass，用于集合阶段之后执行光照计算，该Pass会渲染G缓冲。                 |
| ShadowCaster                 | 用于生成阴影图的Pass，通常用于阴影投射。把物体的深度信息渲染到阴影映射纹理或一张深度纹理中。           |
| MotionVectors                | 用于生成运动矢量的Pass，通常用于运动模糊。                                    |
| Meta                         | 用于在渲染对象元数据时执行的Pass                                         |
| Vertex、VertexLMRGBM、VertexLM | 用于遗留的顶点照明渲染（基本不使用了）                                        |

如果不对LightMode渲染标签进行设置，比如摄像机默认的是前向渲染路径，但是没有为Pass设置相关的标签，那么这个Pass会被当做一个顶点照明渲染路径的Pass。这时光源相关的数据就不会被正确赋值，计算结果就会出现错误，从而呈现出错误的渲染效果。

因此在进行Shader开发时，必须对Pass渲染通道进行正确的LightMode标签设置，从而匹配当前使用的渲染路径。

#### 前向渲染路径

前向渲染路径中会将光源分为以下3中处理方式：

1. 逐像素处理（需要高等质量处理的光）
2. 逐顶点处理（需要中等质量处理的光）
3. 球谐函数（SH）处理（需要低等质量处理的光）

球谐函数处理光照的方式是将光照场景投影到球谐函数的空间中，通过一组球谐系数来表示光照。

> 场景当中的各种光源采用哪种方式处理

在前向渲染中，一部分最亮的灯光以逐像素处理，4个点光源以逐顶点处理，其余灯光以SH处理。

一个光源是逐像素，逐顶点还是SH处理主要取决于以下几点：

1. 渲染模式设置为Not Important的灯光始终以逐顶点或SH方式渲染。
2. 渲染模式设置为Important的灯光始终以逐像素渲染。
3. 最亮的平行光总是以逐像素渲染。
4. 如果逐像素光照的灯光数量少于项目质量设置中的Pixel Light Count，那么其余比较亮的灯光将会以逐像素渲染。

注意：如果灯光渲染模式设置为Auto，Unity会根据灯光的亮度以及与物体的距离自动判断重要性。

> 前向渲染路径处理光照的方式

Unity会对光源进行等级的划分，主要通过灯光渲染模式、项目质量设置中的像素灯光计数、光照强度、距离物体距离来综合判定。

在前向渲染路径中，会将光源分成所谓的逐像素、逐顶点、SH三种处理类型。Unity会将这些光源的数据存储到Shader中对应的内置变量中，我们就可以通过这些内置变量获取到对应的光源数据，从而进行差异化处理。

> 前向渲染路径在哪里进行光照计算

要进行光照计算，肯定是在Shader中的Pass渲染通道中进行计算。但是对于前向渲染来说，有两种Pass可以用来进行光照处理：

1. Base Pass（基础渲染通道）：渲染物体的主要光照通道，用于处理主要的光照效果，主要用于计算逐像素的平行光以及所有逐顶点和SH光源。可实现的效果：漫反射、高光反射、自发光、阴影、光照纹理等。
2. Additional Pass（附加渲染通道）：渲染物体额外的光照通道，用于处理一些附加的光照效果，主要用于计算其他影响物体的逐像素光源，每个光源都会执行一次该Pass。可实现的效果：描边、轮廓、辉光等。

对于一个前向渲染路径下的Shader，通常会定义一个Base Pass以及一个Additional Pass。

每次渲染时，一个Base Pass仅执行一次，而一个Additional Pass会根据影响该物体的其他逐像素光源的数量被多次调用，每个逐像素光源都会调用一次Addtional Pass，由于开启了混合，渲染结果会和之前的光照颜色进行混合。

注意：

1. Base Pass可以有多个，比如需要双面渲染的情况。
2. Base Pass默认支持阴影，Additional Pass默认不支持，可以通过添加#pragma multi_compile_fwdadd_fullshadows编译指令开启阴影。
3. 这些Pass当中处理光照的具体方式是由我们自己决定的，使用逐顶点光照还是逐像素光照的计算方式都根据具体实现而定，前文提到的逐像素光源只是按照期望处理类型来分的而已。

> 前向渲染路径的内置光照变量和函数

常用内置光源变量：

| 变量名                                                    | 类型       | 描述                                   |
| ------------------------------------------------------ | -------- | ------------------------------------ |
| _LightColor0                                           | float4   | 该Pass当前处理的逐像素光源的颜色                   |
| _WorldSpaceLightPos0                                   | float4   | 该Pass当前处理的逐像素光源的位置，如果是平行光，w分量为0，否则为1 |
| _LightMatrix0                                          | float4x4 | 世界空间到光源空间的变换矩阵，可以用于采样光强衰减纹理和cookie   |
| unity_4LightPosX0、 unity_4LightPosY0、unity_4LightPosZ0 | float4   | 仅用于Bass Pass，前4个非重要的点光源在世界空间中的位置     |
| unity_4LightAtten0                                     | float4   | 仅用于Base Pass，存储前4个非重要的点光源的衰减因子       |
| unity_LightColor                                       | half4[4] | 仅用于Base Pass，存储前4个非重要的点光源的颜色         |

常用内置光照函数：

| 函数                      | 描述                              |
| ----------------------- | ------------------------------- |
| WorldSpaceLightDir      | 输入模型空间中的顶点位置，返回世界空间中从该点到光源的光照   |
| UnityWorldSpaceLightDir | 输入世界空间中的顶点位置，返回世界空间中从该点到光源的光照方向 |
| ObjSpaceLightDir        | 输入模型空间中的顶点位置，返回模型空间中从该点到光源的光照方向 |
| Shade4PointLights       | 计算四个点光源的光照，返回四个点光源的叠加颜色         |
#### 顶点照明渲染路径

> 顶点照明渲染路径处理光照的方式

顶点照明渲染路径仅仅是前向渲染路径的一个子集，所有能在顶点照明渲染路径中实现的效果都可以在前向渲染路径中实现。

顶点照明渲染路径对硬件配置要求最少，运算性能最高，但效果最差，其不支持阴影、法线纹理、高精度高光反射等效果。

基本思想是将所有的光都按照逐顶点的方式来进行计算，在内置渲染管线中，它最多只会记录8个光源的数据，根据光源类型、强度、距离等因素来决定。

Unity中的顶点照明渲染路径只会将光相关的数据填充到那些逐顶点相关的内置光源变量中，这意味着我们不能像前向渲染路径那样使用逐像素相关的内置变量。

> 顶点照明渲染路径在哪里进行光照计算

要进行光照计算，肯定是在Shader中的Pass渲染通道里面进行计算。

顶点照明渲染路径通常在一个Pass当中就可以完成对物体的渲染。

在这个Pass当中我们会计算我们关心的所有光源对物体的影响，并且按照逐顶点的方式一次性对所有光照去进行计算。

因此它是Unity内置渲染管线当中最快速的渲染路径，并且具有最广泛的硬件支持，只是渲染效果相对来说最差。

> 内置光照变量和函数

unity_LightColor，unity_LightPosition，unity_LightAtten，unity_SpotDirection

ShaderVertexLights，ShadeVertexLightsFull

#### 延迟渲染路径

> 处理光照的方式

延迟渲染路径**对光照的数量没有任何限制，并且所有灯光都可以采用逐像素渲染**。理论上来说，即使场景中有成百上千个实时灯光，依然可以保持比较流畅的渲染帧率。

**支持法线纹理、阴影等效果处理，但是不能处理半透明物体，并且不支持真正的抗锯齿，这些会自动使用前向渲染路径。**

延迟渲染路径的效率不依赖于场景的复杂度，而是和我们使用的屏幕空间的大小有关。这是因为延迟渲染路径中除了使用颜色缓冲和深度缓冲外，还会利用一个叫做G缓存的额外缓冲区，存储我们关心的表面的其他信息，比如表面的法线、位置、材质属性等等。总之我们需要的信息都存储在缓冲区中，而这些缓冲区可以理解为一张张的2D图片，我们实际上是在这些图像空间中进行处理的。

> 在哪里进行光照计算

延迟渲染路径中主要包含两个Pass：

第一个Pass对于每个物体只会执行一次，通常不需要我们自己实现。主要判断哪些片元可见，并且将可见片元的相关信息存储到G缓冲区中，比如：表面法线、视角方向、漫反射系数等等数据。

第二个Pass中，利用G缓冲区中各个片元的相关信息进行真正的相关计算，最终将颜色写入颜色缓冲区。

注意：第二个Pass中计算光照时，默认情况下只能用Unity内置的标准光照模型计算。

> 内置光照变量

| 变量名                                                                                                                                  | 类型        | 描述                                                                 |
| ------------------------------------------------------------------------------------------------------------------------------------ | --------- | ------------------------------------------------------------------ |
| sampler2D _CameraGBufferTexture0；sampler2D _CameraGBufferTexture1；sampler2D _CameraGBufferTexture2；sampler2D _CameraGBufferTexture3； | sampler2D | 这些变量是自定义变量，我们一般无需实现延迟渲染路径中的第一个Pass，Unity会帮我们将G缓冲中的相关数据存储到这些自定义变量中。 |
| _LightColor                                                                                                                          | float4    | 光源颜色                                                               |
| _LightMatrix0                                                                                                                        | float4x4  | 世界空间到光源空间的变换矩阵                                                     |
#### 渲染路径对比

|        | 前向渲染                                                                                                   | 顶点照明渲染                                                         | 延迟渲染                                                                                 |
| ------ | ------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| 光照处理   | 依据一套规则将光源划分成不同等级，采取不同的处理方式。主要通过灯光渲染模式、项目质量设置中的像素灯光技术的数量、光照强度、距离物体距离来综合判定。                              | 所有灯光均按照逐顶点方式计算，在内置渲染管线中最多记录8个光源的数据，只会将光的相关数据填充到那些逐顶点相关的内置光源变量。 | 对光照的数量没有任何限制，并且所有灯光都可以采用逐像素渲染，不能处理半透明物体，并且不支持真正的抗锯齿。                                 |
| 渲染通道处理 | Bass Pass：处理影响该物体的一个高质量光源（平行光）、所有中（逐顶点）低（SH）质量光源等。Additional Pass：处理影响该物体的除平行光以外的其他高质量光源（每个高质量光源都会调用）。 | 在一个Pass中按照逐顶点的方式一次性计算所有的光照。                                    | 第一个Pass：主要判断片元可见性，将可见片元的相关信息存储到G缓冲区中。第二个Pass：利用G缓冲区中各个片元的相关信息进行真正的相关计算，最终将颜色写入颜色缓冲区。 |
| 优点     | 适用于相对简单的场景和较少数量的光源，基本可以实现任何渲染效果，设备支持率较高。                                                               | 性能开销小，适用于资源受限、设备极差时的轻量级渲染情景。                                   | 适用于复杂场景和大量光源，能够有效减少光照计算的开销。                                                          |
| 缺点     | 对于复杂场景和大量光源的情况，性能消耗相对较大。                                                                               | 表现效果较差，光照计算精度较低。                                               | 对于透明物体和一些特殊效果不能直接支持，需要复杂的处理。对硬件有一定要求，不支持一些性能较差的移动设备。                                 |
### 多种光源

> 常用光源属性

Unity支持的光源类型有：平行光、点光源、聚光灯、面光源。

不管是什么光源类型，在Shader开发中经常用到的光源属性有**位置、方向、颜色、强度、衰减**。

> 平行光

充当角色：太阳

照射范围：无限制

特点：

1. 不存在固定位置
2. 重要属性只有方向
3. 场景中所有点的方向都是一样的
4. 没有衰减（光的强度不会随距离变化）

> 点光源

充当角色：灯泡、烛光等

照射范围：有限

特点：

1. 光是由一个点发出的，向四面八方延申
2. 范围由参数Range来决定
3. 位置由Transform中的Position来决定
4. 存在衰减

> 聚光灯

充当角色：探照灯、手电筒等

照射范围：有限

特点：

1. 光的范围由空间中的一块锥形区域定义
2. 范围由参数Range和Spot Angle共同决定
3. 位置由Transform中的Position来决定
4. 存在衰减，计算公式相比点光源更复杂

#### 如何在Shader中判断光源类型

Unity提供了三个重要的宏：

1. `_DIRECTIONAL_LIGHT`：平行光
2. `_POINT_LIGHT`：点光源
3. `_SPOT_LIGHT`：聚光灯

可以利用这些宏实现条件编译，根据条件判断来包含或排除代码块。

```C
#if defined(_DIRECTIONAL_LIGHT)
	平行光逻辑
#elif defined(_POINT_LIGHT)
	点光源逻辑
#elifd defined(_SPOT_LIGHT)
	聚光灯逻辑
#else
	其他逻辑
#endif
```

Unity底层会根据该条件编译指令，生成多个Shader Variants（着色器变体），这些变体共享相同的核心代码，但根据条件编译的选择会包含不同的代码块。

#### 光照衰减

光源衰减通常指的是在渲染过程中考虑光线在空间中传播时的减弱效应。比如：任何光源的光照强度随着距离的增加都会有所减弱。

一般常见的光照衰减计算方式有：

1. 线性衰减：光强度与距离成线性关系，即光照衰减与光源到被照射表面的距离成正比。
2. 平方衰减：光强度与距离的平方成反比。这种模型更符合现实世界中光照的特性，因为光在空间中的传播过程中通常会遵循平方衰减规律。

> Unity中的光照衰减

为了提升性能，我们一般不会直接通过数学公式计算光照衰减，而是使用一张纹理作为查找表，在片元着色器中计算逐像素光照的衰减。

Unity Shader中有一个内置的纹理类型的变量`_LightTexture0`，该纹理中存储了衰减值相关的数据。对角线上的纹理颜色值，表明了光源空间中不同位置的点对应的衰减值，起点(0,0)位置表示和光源重合的点，终点(1,1)位置表示距离光源最远的点。

一般直接从`LightTexture0`进行纹理采样后，利用其中的`UNITY_ATTEN_CHANNEL`来得到衰减值所在的分量。

注意：如果光源存在cookie（灯光遮罩），那么衰减查找纹理就是`_LightTextureB0`。

> 光源空间变换矩阵

Unity Shader中内置的光源空间变换矩阵是用于将世界空间下的位置转换到光源空间下（光源位置为原点）的。

老版本：`_LightMatrix0`

新版本：`unity_WorldToLight`

由于我们需要从`LightTexture0`光照纹理中取出对应的衰减数据，因此我们需要将顶点位置从世界空间中转换到光源空间中，然后再来从其中取得衰减数据。

#### 点光源衰减计算

 1. 将顶点从世界空间转换到光源空间
 2. 利用该光源空间下的坐标来计算离光源的距离，并利用距离参数，从光源纹理中采样

```C
// lightCoord是光源坐标系下顶点根据光源的范围规范化后的坐标，相当于是一个模长为0~1的向量
float3 lightCoord = mul(unity_WorldToLight, float4(worldPos, 1)).xyz;

// dot(lightCoord, lightCoord)结果是到光源距离的平方distance²
// xx是一种特殊写法，目的是构建一个float2表示uv坐标，相当于(distance², distance²)
// 使用distance²的目的是节省开平方带来的性能消耗，以及贴合现实世界中光照的特性
fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).xx).UNITY_ATTEN_CHANNEL;
```

#### 聚光灯衰减计算

> 聚光灯默认Cookie

聚光灯默认的Cookie主要用于模拟聚光灯的区域性，此时的光照纹理中，`_LightTexture0`存储的是Cookie纹理信息，`_LightTextureB0`存储的是光照纹理信息，里面包含衰减值。
#### 综合实现

```C
Shader "Unlit/Lesson64_ForwardLighting"
{
    Properties
    {
        _MainColor("MainColor", Color) = (1,1,1,1)
        //高光反射颜色  光泽度
        _SpecularColor("SpecularColor", Color) = (1,1,1,1)
        _SpecularNum("SpecularNum", Range(0, 20)) = 1
    }
    SubShader
    {
        //Bass Pass 基础渲染通道
        Pass
        {
            Tags { "LightMode"="ForwardBase" }
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            //用于帮助我们编译所有变体 并且保证衰减相关光照变量能够正确赋值到对应的内置变量中
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "Lighting.cginc"

            //材质漫反射颜色
            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _SpecularNum;

            //顶点着色器返回出去的内容
            struct v2f
            {
                //裁剪空间下的顶点位置
                float4 pos:SV_POSITION;
                //世界空间下的法线位置
                float3 wNormal:NORMAL;
                //世界空间下的 顶点坐标 
                float3 wPos:TEXCOORD0;
            };

            //得到兰伯特光照模型计算的颜色 （逐片元）
            fixed3 getLambertFColor(in float3 wNormal)
            {
                //得到光源单位向量
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);
                //计算除了兰伯特光照的漫反射颜色
                fixed3 color = _LightColor0.rgb * _MainColor.rgb * max(0, dot(wNormal, lightDir));

                return color;
            }

            //得到Blinn Phong式高光反射模型计算的颜色（逐片元）
            fixed3 getSpecularColor(in float3 wPos, in float3 wNormal)
            {
                //1.视角单位向量
                //float3 viewDir = normalize(_WorldSpaceCameraPos.xyz - wPos );
                float3 viewDir = normalize(UnityWorldSpaceViewDir(wPos));

                //2.光的反射单位向量
                //光的方向
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);

                //半角方向向量
                float3 halfA = normalize(viewDir + lightDir);
                
                //color = 光源颜色 * 材质高光反射颜色 * pow( max(0, dot(视角单位向量, 光的反射单位向量)), 光泽度 )
                fixed3 color = _LightColor0.rgb * _SpecularColor.rgb * pow( max(0, dot(wNormal, halfA)), _SpecularNum );

                return color;
            }

            v2f vert (appdata_base v)
            {
                v2f v2fData;
                //转换模型空间下的顶点到裁剪空间中
                v2fData.pos = UnityObjectToClipPos(v.vertex);
                //转换模型空间下的法线到世界空间下
                v2fData.wNormal = UnityObjectToWorldNormal(v.normal);
                //顶点转到世界空间
                v2fData.wPos = mul(unity_ObjectToWorld, v.vertex).xyz;

                return v2fData;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                //计算兰伯特光照颜色
                fixed3 lambertColor = getLambertFColor(i.wNormal);
                //计算BlinnPhong式高光反射颜色
                fixed3 specularColor = getSpecularColor(i.wPos, i.wNormal);

                //衰减值
                fixed atten = 1;
                //物体表面光照颜色 = 环境光颜色 + 兰伯特光照模型所得颜色 + Phong式高光反射光照模型所得颜色
                //衰减值 会和 漫反射颜色 + 高光反射颜色 后 再进行乘法运算
                fixed3 blinnPhongColor = UNITY_LIGHTMODEL_AMBIENT.rgb + (lambertColor + specularColor)*atten; 

                return fixed4(blinnPhongColor.rgb, 1);
            }
            ENDCG
        }

        //Additional Pass 附加渲染通道
        Pass
        {
            Tags { "LightMode"="ForwardAdd" }
            //线性减淡的效果 进行 光照颜色混合
            Blend One One

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            //用于帮助我们编译所有变体 并且保证衰减相关光照变量能够正确赋值到对应的内置变量中
            #pragma multi_compile_fwdadd

            #include "UnityCG.cginc"
            #include "Lighting.cginc"
            #include "AutoLighting.cginc"

            //材质漫反射颜色
            fixed4 _MainColor;
            fixed4 _SpecularColor;
            float _SpecularNum;

            //顶点着色器返回出去的内容
            struct v2f
            {
                //裁剪空间下的顶点位置
                float4 pos:SV_POSITION;
                //世界空间下的法线位置
                float3 wNormal:NORMAL;
                //世界空间下的 顶点坐标 
                float3 wPos:TEXCOORD0;
            };

            v2f vert (appdata_base v)
            {
                v2f v2fData;
                //转换模型空间下的顶点到裁剪空间中
                v2fData.pos = UnityObjectToClipPos(v.vertex);
                //转换模型空间下的法线到世界空间下
                v2fData.wNormal = UnityObjectToWorldNormal(v.normal);
                //顶点转到世界空间
                v2fData.wPos = mul(unity_ObjectToWorld, v.vertex).xyz;

                return v2fData;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                //兰伯特漫反射
                fixed3 worldNormal = normalize(i.wNormal);
                //平行光 光的方向 其实就是它的位置
                #if defined(_DIRECTIONAL_LIGHT)
                    fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz);
                #else //点光源和聚光灯 光的方向 是 光的位置 - 顶点位置
                    fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.wPos);
                #endif
                // 漫反射颜色 = 光颜色 * 属性中颜色 * max(0, dot(世界坐标系下的法线, 世界坐标系下的光方向));
                fixed3 diffuse = _LightColor0.rgb * _MainColor.rgb * max(0, dot(worldNormal, worldLightDir));
                
                //BlinnPhong高光反射
                //视角方向
                fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.wPos.xyz);
                //半角方向向量
                fixed3 halfDir = normalize(worldLightDir + viewDir);
                // 高光颜色 = 光颜色 * 属性中的高光颜色 * pow(max(0, dot(世界坐标系法线, 世界坐标系半角向量)), 光泽度);
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(max(0, dot(worldNormal, halfDir)), _SpecularNum);

                //衰减值
                #ifdef USING_DIRECTIONAL_LIGHT
                    fixed atten = 1;
                #else
                    #if defined(POINT_LIGHT) 
                        //将世界坐标系下顶点转到光源空间下
                        float3 lightCoord = mul(unity_WorldToLight, float4(i.wPos, 1)).xyz;
                        //利用这个坐标得到距离的平方 然后再再光源纹理中隐射得到衰减值
                        fixed atten = tex2D(_LightTexture0, dot(lightCoord,lightCoord).xx).UNITY_ATTEN_CHANNEL;
                    #elif defined(_SPOT_LIGHT)
                        //将世界坐标系下顶点转到光源空间下 聚光灯需要用w参与后续计算
                        float4 lightCoord = mul(unity_WorldToLight, float4(i.wPos, 1));
                        fixed atten = (lightCoord.z > 0) * //判断在聚光灯前面吗
                                    tex2D(_LightTexture0, lightCoord.xy / lightCoord.w + 0.5).w * //映射到大图中进行采样
                                    tex2D(_LightTextureB0, dot(lightCoord,lightCoord).xx).UNITY_ATTEN_CHANNEL; //距离的平方采样
                #else
                    fixed atten = 1;
                #endif

                //在附加渲染通道中不需要在加上环境光颜色了 因为它只需要计算一次 在基础渲染通道中已经计算了
                return fixed4((diffuse + specular)*atten, 1);
            }
            ENDCG
        }
    }
}
```

### 阴影
#### 基本原理

##### 现实中阴影的产生规则

在不考虑光线反射的前提下，当一个光源发射一条光线遇到不透明物体，这条光线就不能继续照射其他物体了，不透明物体会向其他物体投射阴影。

也就是说**阴影区域的产生就是因为光线无法到达**。

##### Shadow Mapping技术

基于上述规则，Lance Williams在1978年提出Shadow Mapping技术。

Shadow Mapping的基本原理：**将摄像机放在与光源重合的位置上，场景中关于该光源的阴影区域就是这个摄像机看不到的地方。**

点光源一般用透视投影，平行光源一般用正交投影。

在Unity中的本质：生成一张深度图（阴影映射纹理），一般存放在显存中，这张图记录了从该光源出发，摄像机能看到的场景中距离它最近表面的位置（0最近，1最远）。

##### 在Unity中如何应用Shadow Mapping技术

阴影映射纹理的生成是由光源来完成的。

在渲染的早期阶段，Unity会为每个光源创建一个摄像机视角，点光源可能会设置多个视角，捕获多个方向的立方体阴影贴图，最后将深度信息记录到阴影映射纹理中。

实时阴影映射需要每帧更新，但是对于静态光源和静态场景，可以使用预烘焙的阴影贴图，减少实时计算的开销。

有了阴影映射纹理之后，就可以在Pass中将顶点位置变换到光源空间下，使用XY分量对阴影映射纹理采样得到深度值，如果深度值小于顶点的深度值，说明该顶点位于阴影中。

#### Unity中实现阴影的原理

##### Screen Space Shadow Mapping 技术

Unity并不纯粹地使用Shadow Mapping 技术，还会使用Screen Space Shadow Mapping 技术。

该技术由微软研究院于2011年首次提出，是对Shadow Mapping的一种改进和拓展。

不是所有设备都支持SSSM，Unity会在内部帮我们判断对应平台是否支持，若不支持则会使用Shadow Mapping。

SSSM在SM的基础上，还会生成一张屏幕空间深度图，记录从摄像机看到的每个像素（对应场景中的顶点）的深度值。

将屏幕空间的像素位置转换到光源空间下，然后在光源空间下比较每个像素的深度值和阴影映射纹理中的值，如果当前像素深度值大于光源深度图中的值，说明在阴影中。

##### Unity中如何实现阴影

当一个Pass的LightMode被设置为Shadow Caster，Unity就会生成对应的阴影映射纹理。

如果当前Shader中没有Shadow Caster，那么就会在Fallback指定的Shader中一层一层地往下寻找。

屏幕空间深度图通常由摄像机在渲染过程中自动生成，并存储在摄像机的深度纹理中。

想要让Unity中的物体接受阴影和投射阴影，就需要对光源和物体进行一些设置：

1. 在光源组件上设置Shadow Type
2. 在网格渲染器中勾选Receive Shadow
3. 在网格渲染器中勾选Cast Shadow

#### 不透明物体投射阴影

1. 添加一个LightMode为ShadowCaster的Pass
2. 设置编译指令：#pragma multi_compile_shadowcaster。用于支持不同类型的阴影（SM，SSSM等）
3. 引用内置文件：#include "UnityCG.inc"
4. 在顶点着色器输出结构体中定义宏：V2F_SHADOW_CASTER
5. 在顶点着色器中使用宏：TRANSFER_SHADOW_CASTER_NORMALOFFSET(v2f o)
6. 在片元着色器中使用宏：SHADOW_CASTER_FRAGMENT(i)

```CS
Pass
{
	Tags
	{
		"LightMode" = "ShadowCaster"
	}
	CGPROGRAM
	#pragma vertex vert
	#pragma fragment frag
	#pragma multi_compile_shadowcaster
	
	#include "UnityCG.cginc"

	struct v2f
	{
		V2F_SHADOW_CASTER;
	};
	
	v2f vert(appdata_base v)
	{
		v2f o;
		TRANSFER_SHADOW_CASTER_NORMALOFFSET(o);
		return o;
	}

	float4 frag(v2f i) : SV_Target
	{
		SHADOW_CASTER_FRAGMENT(i);
	}
	ENDCG
}
```

#### 不透明物体接收阴影

主要流程：

1. 在顶点着色器中进行坐标转换，将顶点坐标转换成阴影映射纹理坐标
2. 在片元着色器中使用阴影映射纹理坐标对阴影映射纹理进行采样，通过得到的深度值判断片元是否在阴影当中，以获得阴影衰减值
3. 将采样结果应用到最终颜色计算中

##### 具体步骤

1. 在v2f结构体中使用SHADOW_COORD(i)，i是未使用过的TEXCOORD索引值
2. 在顶点着色器中使用TRANSFER_SHADOW(v2f o)，它会在内部判断应该使用哪种阴影映射技术，对顶点坐标进行转换并存储到_ShadowCoord变量中。
3. 在片元着色器中使用SHADOW_ATTENUATION得到阴影衰减值

注意：输入结构体中顶点位置变量命名必须是vertex，输出结构体中顶点位置命名必须是pos。

#### 光源和阴影衰减综合实现

```CS
Shader "Unlit/Lesson68_Attenuation"  
{  
   Properties  
    {  
        _MainColor("MainColor", Color) = (1,1,1,1)  
        _SpecularColor("SpecularColor", Color) = (1,1,1,1)  
        _SpecularNum("SpecularNum", Range(0, 20)) = 1  
    }  
    SubShader  
    {  
        Pass  
        {  
            Tags { "LightMode"="ForwardBase" }  
            CGPROGRAM  
            #pragma vertex vert  
            #pragma fragment frag  
            #pragma multi_compile_fwdbase  
  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
            #include "AutoLight.cginc"  
            fixed4 _MainColor;  
            fixed4 _SpecularColor;  
            float _SpecularNum;  
            struct v2f  
            {  
                float4 pos:SV_POSITION;  
                float3 wNormal:NORMAL;  
                float3 wPos:TEXCOORD0;  
                SHADOW_COORDS(2)  
            };  
            fixed3 getLambertFColor(in float3 wNormal)  
            {                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);  
                fixed3 color = _LightColor0.rgb * _MainColor.rgb * max(0, dot(wNormal, lightDir));  
  
                return color;  
            }  
            fixed3 getSpecularColor(in float3 wPos, in float3 wNormal)  
            {                float3 viewDir = normalize(UnityWorldSpaceViewDir(wPos));  
                float3 lightDir = normalize(_WorldSpaceLightPos0.xyz);  
                float3 halfA = normalize(viewDir + lightDir);  
                fixed3 color = _LightColor0.rgb * _SpecularColor.rgb * pow(max(0, dot(wNormal, halfA)), _SpecularNum);  
  
                return color;  
            }  
            v2f vert (appdata_base v)  
            {                v2f v2fData;  
                v2fData.pos = UnityObjectToClipPos(v.vertex);  
                v2fData.wNormal = UnityObjectToWorldNormal(v.normal);  
                v2fData.wPos = mul(unity_ObjectToWorld, v.vertex).xyz;  
  
                TRANSFER_SHADOW(v2fData);  
  
                return v2fData;  
            }  
            fixed4 frag (v2f i) : SV_Target  
            {  
                fixed3 lambertColor = getLambertFColor(normalize(i.wNormal));  
                fixed3 specularColor = getSpecularColor(i.wPos, normalize(i.wNormal));  
                UNITY_LIGHT_ATTENUATION(atten, i, i.wPos);  
                fixed3 blinnPhongColor = UNITY_LIGHTMODEL_AMBIENT.rgb + (lambertColor + specularColor) * atten;   
                return fixed4(blinnPhongColor.rgb, 1);  
            }            ENDCG  
        }  
  
        Pass  
        {  
            Tags { "LightMode"="ForwardAdd" }  
            Blend One One  
  
            CGPROGRAM            #pragma vertex vert  
            #pragma fragment frag  
            #pragma multi_compile_fwdadd_fullshadows  
  
            #include "UnityCG.cginc"  
            #include "Lighting.cginc"  
            #include "AutoLight.cginc"  
  
            fixed4 _MainColor;  
            fixed4 _SpecularColor;  
            float _SpecularNum;  
  
            struct v2f  
            {  
                float4 pos:SV_POSITION;  
                float3 wNormal:NORMAL;  
                float3 wPos:TEXCOORD0;  
                SHADOW_COORDS(2)  
            };  
            v2f vert (appdata_base v)  
            {                v2f v2fData;  
                v2fData.pos = UnityObjectToClipPos(v.vertex);  
                v2fData.wNormal = UnityObjectToWorldNormal(v.normal);  
                v2fData.wPos = mul(unity_ObjectToWorld, v.vertex).xyz;  
  
                TRANSFER_SHADOW(v2fData);  
  
                return v2fData;  
            }  
            fixed4 frag (v2f i) : SV_Target  
            {  
                fixed3 worldNormal = normalize(i.wNormal);  
                #if defined(_DIRECTIONAL_LIGHT)  
                    fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz);  
                #else  
                    fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.wPos);  
                #endif  
                fixed3 diffuse = _LightColor0.rgb * _MainColor.rgb * max(0, dot(worldNormal, worldLightDir));  
                fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.wPos.xyz);  
                fixed3 halfDir = normalize(worldLightDir + viewDir);  
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(max(0, dot(worldNormal, halfDir)), _SpecularNum);  
  
                UNITY_LIGHT_ATTENUATION(atten, i, i.wPos)  
  
                return fixed4((diffuse + specular) * atten, 1);  
            }            ENDCG  
        }  
    }  
    FallBack "Specular"  
}
```

#### 透明物体阴影

##### 透明度测试

```CS
Shader "Unlit/TransparentCutout"
{
    Properties
    {
        _Color("Color", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "white" {}
        _SpecularColor("Specular", Color) = (1,1,1,1)
        _Shininess("Shininess", Float) = 20
        _Cutoff("Alpha Cutoff", Range(0.0, 1.0)) = 0.5
    }
    SubShader
    {
        Tags
        {
            "LightMode"="ForwardBase"
            "RenderType"="TransparentCutout"
            "IgnoreProjector"="True"
        }
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "Lighting.cginc"
            #include "AutoLight.cginc"

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
                float4 worldVertex : TEXCOORD1;
                half3 worldNormal : TEXCOORD2;
                SHADOW_COORDS(3)
            };

            half3 _Color;
            half3 _SpecularColor;
            float _Shininess;

            sampler2D _MainTex;
            float4 _MainTex_ST;

            fixed _Cutoff;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.texcoord, _MainTex);

                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);

                o.worldNormal = UnityObjectToWorldNormal(v.normal);

                TRANSFER_SHADOW(o)
                return o;
            }

            half getLambert(in half3 worldNormal, in half3 lightDir)
            {
                return dot(worldNormal, lightDir) * 0.5 + 0.5;
            }

            half getBlinnPhong(in half3 worldNormal, in half3 lightDir, in half3 viewDir)
            {
                half3 halfDir = normalize(lightDir + viewDir);
                return pow(saturate(dot(worldNormal, halfDir)), _Shininess);
            }

            fixed4 frag(v2f i) : SV_Target
            {
                half4 albedo = tex2D(_MainTex, i.uv);
                clip(albedo.a - _Cutoff);

                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));
                half3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldVertex));

                half3 ambient = unity_AmbientSky.rgb;
                half3 diffuse = _LightColor0.rgb * _Color.rgb * albedo.rgb * getLambert(i.worldNormal, lightDir);
                half3 specular = _LightColor0.rgb * _SpecularColor.rgb *
                    getBlinnPhong(i.worldNormal, lightDir, viewDir);

                UNITY_LIGHT_ATTENUATION(atten, i, i.worldVertex)

                half3 color = diffuse + specular;
                color *= atten;

                return half4(color + ambient * albedo.rgb, 1);
            }
            ENDCG
        }
    }
    Fallback "Legacy Shaders/Transparent/Cutout/Diffuse"
}
```

##### 透明度混合

由于透明物体计算阴影是非常复杂的，出于性能考虑，Unity用于计算阴影的宏不会为开启透明度混合的Shader计算阴影。

### 标准光照着色器

#### 标准漫反射着色器

```CS
Shader "Unlit/BumpedDiffuse"
{
    Properties
    {
        _DiffuseColor("MainColor", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "white" {}
        _BumpTex("Bump Texture", 2D) = "white" {}
        _BumpScale("Bump Scale", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "RenderType"="Opaque"
            "Queue"="Geometry"
        }
        Pass
        {
            Tags
            {
                "LightMode"="ForwardBase"
            }
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include  "Lighting.cginc"
            #include "AutoLight.cginc"

            struct v2f
            {
                float4 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
                float3 vertex: TEXCOORD1;
                half3 t2w0: TEXCOORD2;
                half3 t2w1: TEXCOORD3;
                half3 t2w2: TEXCOORD4;
                SHADOW_COORDS(5)
            };

            fixed4 _DiffuseColor;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpTex;
            float4 _BumpTex_ST;
            fixed _BumpScale;

            v2f vert(appdata_full i)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(i.vertex);
                o.uv.xy = TRANSFORM_TEX(i.texcoord, _MainTex);
                o.uv.zw = TRANSFORM_TEX(i.texcoord, _BumpTex);

                o.vertex = mul(unity_ObjectToWorld, i.vertex);

                half3 worldNormal = UnityObjectToWorldNormal(i.normal);
                half3 worldTangent = UnityObjectToWorldDir(i.tangent.xyz);
                half3 worldBitangent = cross(worldNormal, worldTangent) * i.tangent.w;

                o.t2w0 = half3(worldTangent.x, worldBitangent.x, worldNormal.x);
                o.t2w1 = half3(worldTangent.y, worldBitangent.y, worldNormal.y);
                o.t2w2 = half3(worldTangent.z, worldBitangent.z, worldNormal.z);

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                // 将法线从切线空间转换到世界空间
                half3 normal = UnpackNormal(tex2D(_BumpTex, i.uv.zw));
                normal.xy *= _BumpScale;
                normal.z = sqrt(1 - dot(normal.xy, normal.xy));
                normal = half3(dot(i.t2w0, normal), dot(i.t2w1, normal), dot(i.t2w2, normal));

                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));

                fixed4 albedo = tex2D(_MainTex, i.uv.xy) * _DiffuseColor;
                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.rgb * albedo.rgb;
                fixed3 diffuse = _LightColor0.rgb * albedo.rgb * saturate(dot(normal, lightDir));

                UNITY_LIGHT_ATTENUATION(atten, i, i.vertex)

                return fixed4(ambient + diffuse * atten, 1);
            }
            ENDCG
        }
        Pass
        {
            Tags
            {
                "LightMode"="ForwardAdd"
            }
            Blend One One
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdadd

            #include "UnityCG.cginc"
            #include "Lighting.cginc"
            #include "AutoLight.cginc"

            struct v2f
            {
                float4 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
                float3 vertex: TEXCOORD1;
                half3 t2w0: TEXCOORD2;
                half3 t2w1: TEXCOORD3;
                half3 t2w2: TEXCOORD4;
                SHADOW_COORDS(5)
            };

            fixed4 _DiffuseColor;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpTex;
            float4 _BumpTex_ST;
            fixed _BumpScale;

            v2f vert(appdata_full i)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(i.vertex);
                o.uv.xy = TRANSFORM_TEX(i.texcoord, _MainTex);
                o.uv.zw = TRANSFORM_TEX(i.texcoord, _BumpTex);

                o.vertex = mul(unity_ObjectToWorld, i.vertex);

                half3 worldNormal = UnityObjectToWorldNormal(i.normal);
                half3 worldTangent = UnityObjectToWorldDir(i.tangent.xyz);
                half3 worldBitangent = cross(worldNormal, worldTangent) * i.tangent.w;

                o.t2w0 = half3(worldTangent.x, worldBitangent.x, worldNormal.x);
                o.t2w1 = half3(worldTangent.y, worldBitangent.y, worldNormal.y);
                o.t2w2 = half3(worldTangent.z, worldBitangent.z, worldNormal.z);

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                // 将法线从切线空间转换到世界空间
                half3 normal = UnpackNormal(tex2D(_BumpTex, i.uv.zw));
                normal.xy *= _BumpScale;
                normal.z = sqrt(1 - dot(normal.xy, normal.xy));
                normal = half3(dot(i.t2w0, normal), dot(i.t2w1, normal), dot(i.t2w2, normal));

                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));

                fixed4 albedo = tex2D(_MainTex, i.uv.xy) * _DiffuseColor;
                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.rgb * albedo.rgb;
                fixed3 diffuse = _LightColor0.rgb * albedo.rgb * saturate(dot(normal, lightDir));

                UNITY_LIGHT_ATTENUATION(atten, i, i.vertex)

                return fixed4(ambient + diffuse * atten, 1);
            }
            ENDCG
        }
    }
    Fallback "Diffuse"
}
```

#### 标准镜面反射着色器

```CS
Shader "Unlit/BumpedSpecular"
{
    Properties
    {
        _DiffuseColor("Main Color", Color) = (1,1,1,1)
        _SpecularColor("Specular Color", Color) = (1,1,1,1)
        _Shininess("Shininess", Range(0.1, 200)) = 100
        _MainTex ("Texture", 2D) = "white" {}
        _BumpTex("Bump Texture", 2D) = "white" {}
        _BumpScale("Bump Scale", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "RenderType"="Opaque"
            "Queue"="Geometry"
        }
        Pass
        {
            Tags
            {
                "LightMode"="ForwardBase"
            }
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include  "Lighting.cginc"
            #include "AutoLight.cginc"

            struct v2f
            {
                float4 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
                float3 vertex: TEXCOORD1;
                half3 t2w0: TEXCOORD2;
                half3 t2w1: TEXCOORD3;
                half3 t2w2: TEXCOORD4;
                SHADOW_COORDS(5)
            };

            fixed4 _DiffuseColor;
            fixed4 _SpecularColor;
            fixed _Shininess;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpTex;
            float4 _BumpTex_ST;
            fixed _BumpScale;

            v2f vert(appdata_full i)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(i.vertex);
                o.uv.xy = TRANSFORM_TEX(i.texcoord, _MainTex);
                o.uv.zw = TRANSFORM_TEX(i.texcoord, _BumpTex);

                o.vertex = mul(unity_ObjectToWorld, i.vertex);

                half3 worldNormal = UnityObjectToWorldNormal(i.normal);
                half3 worldTangent = UnityObjectToWorldDir(i.tangent.xyz);
                half3 worldBitangent = cross(worldNormal, worldTangent) * i.tangent.w;

                o.t2w0 = half3(worldTangent.x, worldBitangent.x, worldNormal.x);
                o.t2w1 = half3(worldTangent.y, worldBitangent.y, worldNormal.y);
                o.t2w2 = half3(worldTangent.z, worldBitangent.z, worldNormal.z);

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                // 将法线从切线空间转换到世界空间
                half3 normal = UnpackNormal(tex2D(_BumpTex, i.uv.zw));
                normal.xy *= _BumpScale;
                normal.z = sqrt(1 - dot(normal.xy, normal.xy));
                normal = half3(dot(i.t2w0, normal), dot(i.t2w1, normal), dot(i.t2w2, normal));

                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));
                half3 viewDir = normalize(UnityWorldSpaceViewDir(i.vertex));
                half3 halfDir = normalize(lightDir + viewDir);

                fixed4 albedo = tex2D(_MainTex, i.uv.xy) * _DiffuseColor;
                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.rgb * albedo.rgb;
                fixed3 diffuse = _LightColor0.rgb * albedo.rgb * saturate(dot(normal, lightDir));
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(
                    saturate(dot(normal, halfDir)), _Shininess);

                UNITY_LIGHT_ATTENUATION(atten, i, i.vertex)

                return fixed4(ambient + (diffuse + specular) * atten, 1);
            }
            ENDCG
        }
        Pass
        {
            Tags
            {
                "LightMode"="ForwardAdd"
            }
            Blend One One
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdadd

            #include "UnityCG.cginc"
            #include  "Lighting.cginc"
            #include "AutoLight.cginc"

            struct v2f
            {
                float4 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
                float3 vertex: TEXCOORD1;
                half3 t2w0: TEXCOORD2;
                half3 t2w1: TEXCOORD3;
                half3 t2w2: TEXCOORD4;
                SHADOW_COORDS(5)
            };

            fixed4 _DiffuseColor;
            fixed4 _SpecularColor;
            fixed _Shininess;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpTex;
            float4 _BumpTex_ST;
            fixed _BumpScale;

            v2f vert(appdata_full i)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(i.vertex);
                o.uv.xy = TRANSFORM_TEX(i.texcoord, _MainTex);
                o.uv.zw = TRANSFORM_TEX(i.texcoord, _BumpTex);

                o.vertex = mul(unity_ObjectToWorld, i.vertex);

                half3 worldNormal = UnityObjectToWorldNormal(i.normal);
                half3 worldTangent = UnityObjectToWorldDir(i.tangent.xyz);
                half3 worldBitangent = cross(worldNormal, worldTangent) * i.tangent.w;

                o.t2w0 = half3(worldTangent.x, worldBitangent.x, worldNormal.x);
                o.t2w1 = half3(worldTangent.y, worldBitangent.y, worldNormal.y);
                o.t2w2 = half3(worldTangent.z, worldBitangent.z, worldNormal.z);

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                // 将法线从切线空间转换到世界空间
                half3 normal = UnpackNormal(tex2D(_BumpTex, i.uv.zw));
                normal.xy *= _BumpScale;
                normal.z = sqrt(1 - dot(normal.xy, normal.xy));
                normal = half3(dot(i.t2w0, normal), dot(i.t2w1, normal), dot(i.t2w2, normal));

                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.vertex));
                half3 viewDir = normalize(UnityWorldSpaceViewDir(i.vertex));
                half3 halfDir = normalize(lightDir + viewDir);

                fixed4 albedo = tex2D(_MainTex, i.uv.xy) * _DiffuseColor;
                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.rgb * albedo.rgb;
                fixed3 diffuse = _LightColor0.rgb * albedo.rgb * saturate(dot(normal, lightDir));
                fixed3 specular = _LightColor0.rgb * _SpecularColor.rgb * pow(
                    saturate(dot(normal, halfDir)), _Shininess);

                UNITY_LIGHT_ATTENUATION(atten, i, i.vertex)

                return fixed4(ambient + (diffuse + specular) * atten, 1);
            }
            ENDCG
        }
    }
    Fallback "Specular"
}
```

## 高级纹理

### 立方体纹理

立方体纹理包含6张独立的2D纹理，分别对应立方体的六个面。

##### 用途

1. 环境映射：模拟环境反射，用立方体纹理存储周围环境的图像
2. 天空盒：渲染天空盒，将立方体纹理映射到立方体的内表面，模拟天空、山脉等环境。
3. 全景图：展示全景图像、视频。

##### 如何采样

对立方体纹理采样，需要提供一个三维坐标，该坐标表示我们在世界空间下的一个方向向量，利用这个向量从立方体中心出发，往外延申就会与立方体表面产生交点，交点处的纹理信息就是采样结果。

##### 优缺点

优点：

1. 多用途：用于模拟天空盒、反射、折射、环境光照等。
2. 无缝连接
3. 兼容性好

缺点：

1. 内存开销大
2. 采样复杂
3. 透视变形

#### 天空盒

1. 创建材质，使用Skybox/6 sided着色器
2. 将六个面的纹理赋值到材质上，纹理的 Wrap Mode 设置为 Clamp
3. 在Lighting面板中更改天空盒材质，全局生效
4. 在Camera上添加Skybox组件设置天空盒材质，单独一个摄像机生效

#### 动态生成立方体纹理

对于天空盒来说，其使用的立方体纹理都是提前生成的。

但是对于场景中的每一个物体，如果依然使用这种提前生成的纹理，那么在实现反射效果的时候就无法发射出环境中其他动态物体的图像。

因此，为了实现比较真实的效果，就需要动态地为每一个位置的物体生成对应的立方体纹理。

#### 反射

反射是指光在两种物质的分界面上改变了传播方向，返回原物质中的现象。

在着色器中实现反射效果的方法：利用视角方向向量的反射向量在立方体纹理中采样，得到反射的颜色。

##### 基础实现

```CS
Shader "Unlit/ReflectionBase"
{
    Properties
    {
        _Cubemap("Cubemap", Cube) = "" {}
        _Reflectivity("Reflectivity", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "LightMode"="ForwardBase"
            "RenderType"="Opaque"
        }

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                half3 reflectionDir: TEXCOORD0;
            };

            samplerCUBE _Cubemap;
            fixed _Reflectivity;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                float3 worldVertex = mul(unity_ObjectToWorld, v.vertex);
                half3 worldNormal = UnityObjectToWorldNormal(v.normal);
                half3 viewDir = UnityWorldSpaceViewDir(worldVertex);
                o.reflectionDir = reflect(-viewDir, worldNormal);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                fixed4 reflectionColor = texCUBE(_Cubemap, i.reflectionDir) * _Reflectivity;

                return reflectionColor;
            }
            ENDCG
        }
    }
}
```

##### 结合漫反射实现

```CS
Shader "Unlit/ReflectionDiffuse"
{
    Properties
    {
        _DiffuseColor("Diffuse Color", Color) = (1,1,1,1)
        _ReflectionColor("Reflection Color", Color) = (1,1,1,1)
        _Cubemap("Cubemap", Cube) = "" {}
        _Reflectivity("Reflectivity", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "LightMode"="ForwardBase"
            "RenderType"="Opaque"
        }
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "AutoLight.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float3 worldVertex: TEXCOORD0;
                half3 worldNormal: TEXCOORD1;
                half3 reflectionDir: TEXCOORD2;
                SHADOW_COORDS(3)
            };

            fixed4 _DiffuseColor;
            fixed4 _ReflectionColor;
            samplerCUBE _Cubemap;
            fixed _Reflectivity;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);
                o.worldNormal = UnityObjectToWorldNormal(v.normal);
                half3 viewDir = normalize(UnityWorldSpaceViewDir(o.worldVertex));
                o.reflectionDir = reflect(-viewDir, o.worldNormal);
                TRANSFER_SHADOW(o);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));
                fixed3 diffuseColor = _LightColor0.rgb * _DiffuseColor.rgb * saturate(dot(i.worldNormal, lightDir));
                fixed3 reflectionColor = texCUBE(_Cubemap, i.reflectionDir).rgb * _ReflectionColor.rgb;
                UNITY_LIGHT_ATTENUATION(atten, i, i.worldVertex);
                fixed3 color = lerp(diffuseColor, reflectionColor, _Reflectivity) * atten + unity_AmbientSky.rgb;
                return fixed4(color, 1);
            }
            ENDCG
        }
    }
    Fallback "Legacy Shaders/Reflective/Diffuse"
}
```

#### 折射

折射是指光从一种透明介质倾斜地进入到另一种透明介质中时，由于光在两种介质中的传播速度不同导致传播方向发生改变的现象。

一般会利用折射来模拟水面、玻璃、空气扰动等效果。

在着色器中实现折射效果的方法：计算出视角方向向量的折射向量在立方体纹理中采样，得到折射的颜色。

> 斯涅尔定律：$n_1 sin(\theta_1)=n_2 sin(\theta_2)$，n是介质的折射率，$\theta$是光线和法线的夹角

现实中光穿过一个物体其实会发生两次折射，但是在实时渲染中模拟第二次折射是比较复杂的，所以一般只用计算一次折射即可，只要视觉效果能接受就行了。

```CS
Shader "Unlit/RefractionDiffuse"
{
    Properties
    {
        _DiffuseColor("Diffuse Color", Color) = (1,1,1,1)
        _RefractionColor("Reflection Color", Color) = (1,1,1,1)
        _Cubemap("Cubemap", Cube) = "" {}
        _RefractRatio("RefractRatio", Range(0.1, 1)) = 0.7
        _RefractAmount("RefractAmount", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "LightMode"="ForwardBase"
            "RenderType"="Opaque"
        }
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "AutoLight.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float3 worldVertex: TEXCOORD0;
                half3 worldNormal: TEXCOORD1;
                half3 refractionDir: TEXCOORD2;
                SHADOW_COORDS(3)
            };

            fixed4 _DiffuseColor;
            fixed4 _RefractionColor;
            samplerCUBE _Cubemap;
            fixed _RefractRatio;
            fixed _RefractAmount;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);
                o.worldNormal = UnityObjectToWorldNormal(v.normal);
                half3 viewDir = normalize(UnityWorldSpaceViewDir(o.worldVertex));
                o.refractionDir = refract(-viewDir, o.worldNormal, _RefractRatio);
                TRANSFER_SHADOW(o);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));
                fixed3 diffuseColor = _LightColor0.rgb * _DiffuseColor.rgb * saturate(dot(i.worldNormal, lightDir));
                fixed3 refractionColor = texCUBE(_Cubemap, i.refractionDir).rgb * _RefractionColor.rgb;
                UNITY_LIGHT_ATTENUATION(atten, i, i.worldVertex);
                fixed3 color = lerp(diffuseColor, refractionColor, _RefractAmount) * atten + unity_AmbientSky.rgb;
                return fixed4(color, 1);
            }
            ENDCG
        }
    }
    Fallback "Legacy Shaders/Reflective/Diffuse"
}
```

#### 菲涅尔反射

当视线垂直于表面时，看到的反射效果较弱，当视线与表面形成锐角时，角度越小，看到的反射效果越明显。

菲涅尔反射描述的是光在两种不同介质的表面发生的折射和反射现象，其强度取决于光的入射角和介质的折射率。

所有物体在光的照射下都会产生菲涅尔反射现象。

菲涅尔等式是复杂的，因此在实时渲染中采用Schlick菲涅尔近似等式来计算反射率。

> Schlick菲涅尔近似等式：$R(\theta)=R_0+(1-R_0){(1-cos(\theta))}{5}$，$\theta$是入射角度，$R_0$是垂直于入射表面时的反射率

```CS
Shader "Unlit/FresnelReflectionDiffuse"
{
    Properties
    {
        _DiffuseColor("Diffuse Color", Color) = (1,1,1,1)
        _ReflectionColor("Reflection Color", Color) = (1,1,1,1)
        _Cubemap("Cubemap", Cube) = "" {}
        _FresnelScale("FresnelScale", Range(0,1)) = 1
    }
    SubShader
    {
        Tags
        {
            "LightMode"="ForwardBase"
            "RenderType"="Opaque"
        }
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "AutoLight.cginc"
            #include "Lighting.cginc"

            struct v2f
            {
                float4 pos : SV_POSITION;
                float3 worldVertex: TEXCOORD0;
                half3 worldNormal: TEXCOORD1;
                half3 reflectionDir: TEXCOORD2;
                SHADOW_COORDS(3)
                float reflectivity: TEXCOORD4;
            };

            fixed4 _DiffuseColor;
            fixed4 _ReflectionColor;
            samplerCUBE _Cubemap;
            fixed _FresnelScale;

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.worldVertex = mul(unity_ObjectToWorld, v.vertex);
                o.worldNormal = UnityObjectToWorldNormal(v.normal);
                half3 viewDir = normalize(UnityWorldSpaceViewDir(o.worldVertex));
                o.reflectionDir = reflect(-viewDir, o.worldNormal);
                o.reflectivity = _FresnelScale + (1 - _FresnelScale) * pow(1 - dot(viewDir, o.worldNormal), 5);
                TRANSFER_SHADOW(o);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                half3 lightDir = normalize(UnityWorldSpaceLightDir(i.worldVertex));
                fixed3 diffuseColor = _LightColor0.rgb * _DiffuseColor.rgb * saturate(dot(i.worldNormal, lightDir));
                fixed3 reflectionColor = texCUBE(_Cubemap, i.reflectionDir).rgb * _ReflectionColor.rgb;
                UNITY_LIGHT_ATTENUATION(atten, i, i.worldVertex);
                fixed3 color = lerp(diffuseColor, reflectionColor, i.reflectivity) * atten + unity_AmbientSky.rgb;
                return fixed4(color, 1);
            }
            ENDCG
        }
    }
    Fallback "Legacy Shaders/Reflective/Diffuse"
}
```
### 渲染纹理

### 程序纹理

